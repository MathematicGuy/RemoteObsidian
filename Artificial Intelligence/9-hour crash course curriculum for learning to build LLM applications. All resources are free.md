ğ—£ğ—”ğ—¥ğ—§ ğŸ­ - ğ—” ğ—šğ—˜ğ—¡ğ—§ğ—Ÿğ—˜ ğ—œğ—¡ğ—§ğ—¥ğ—¢ ğ—§ğ—¢ ğ—Ÿğ—Ÿğ— ğ—¦ (1h 30m)
Intro & overview of language models, next-word prediction, embeddings, cosine similarity, semantic search
Resources: 
ğŸ“º 30m: Simple Introduction to LLMs (Matthew Berman) bit.ly/4bBzsOn
ğŸ“– 40m: A Very Gentle Introduction to LLMs without the Hype (Mark Riedl) bit.ly/3x7g1xD
ğŸ“º 10m: How do LLMs work? Next Word Prediction with the Transformer Architecture Explained (Louis-FranÃ§ois Bouchard) bit.ly/4e553JY
ğŸ“º 10m: What is Semantic Search? (Luis Serrano) bit.ly/4bUAX9O
---
ğ—£ğ—”ğ—¥ğ—§ ğŸ® - ğ—§ğ—¥ğ—”ğ—¡ğ—¦ğ—™ğ—¢ğ—¥ğ— ğ—˜ğ—¥ ğ—”ğ—¥ğ—–ğ—›ğ—œğ—§ğ—˜ğ—–ğ—§ğ—¨ğ—¥ğ—˜ (1h 30m)
Encoder-decoder architecture, masking, attention, transformers, GPTs
Resources:
ğŸ“º 30m: But what is a GPT? Visual intro to transformers (3Blue1Brown) bit.ly/452JZQ6
ğŸ“º 20m: Sequence-to-Sequence (seq2seq) Encoder-Decoder Neural Networks, Clearly Explained! (Josh Starmer) bit.ly/4c08M9z
ğŸ“º 10m: The Attention Mechanism (Luis Serrano) bit.ly/4e5O75N
ğŸ“º 15m: Transformer Models (Luis Serrano) bit.ly/3WYVvdj
ğŸ“º 15m: Generative Pre-trained Transformer (GPT) (Databricks) bit.ly/3VmAMil
---
ğ—£ğ—”ğ—¥ğ—§ ğŸ¯ - ğ—£ğ—¥ğ—¢ğ— ğ—£ğ—§ ğ—˜ğ—¡ğ—šğ—œğ—¡ğ—˜ğ—˜ğ—¥ğ—œğ—¡ğ—š (2h 0m)
Zero/one/few-shot, chain-of-thought, self-consistency, generated knowledge, prompt chaining, ReAct
Resources:
ğŸ“º 60m: Prompt Engineering Overview (Elvis Saravia) bit.ly/3V3FIXV
ğŸ“– 90m: Prompt Engineering Guide (DAIR) bit.ly/3Pf1dCV
ğŸ“– 30m: Brex's Prompt Engineering Guide (Brex) bit.ly/3V2elO6
âš’ï¸ 0m: LangChain Hub bit.ly/4c0wpiw
---
ğ—£ğ—”ğ—¥ğ—§ ğŸ° - ğ—–ğ—›ğ—”ğ—œğ—¡ğ—œğ—¡ğ—š, ğ—¥ğ—”ğ—š, ğ—©ğ—˜ğ—–ğ—§ğ—¢ğ—¥ ğ——ğ—•ğ—¦, ğ—”ğ—šğ—˜ğ—¡ğ—§ğ—¦ (2h 30m)
Langchain, RAG, vector databases, LlamaIndex, Open AI functions
Resources:
ğŸ“º 30m: The LangChain Cookbook - Beginner Guide To 7 Essential Concepts (Greg Kamradt) bit.ly/4e2ozX9
ğŸ“º 30m: The LangChain Cookbook Part 2 - Beginner Guide To 9 Use Cases (Greg Kamradt) bit.ly/4bQoMLo
âš’ï¸ 0m: Learn LangChain (Greg Kamradt) bit.ly/455XTRB
ğŸ“– 30m: Advanced RAG (Hugging Face) bit.ly/3KmzkpO
ğŸ“º 5m: Vector databases are so hot right now. WTF are they? (Fireship) bit.ly/4aMqRH8
ğŸ“º 10m: Question A 300 Page Book (w/ OpenAI + Pinecone) (Greg Kamradt) bit.ly/4bRGzSA
ğŸ“º 20m: Talk to Your Documents, Powered by Llama-Index (Prompt Engineering) bit.ly/4bZ6K9F
ğŸ“º 30m: From OpenAI Function Calling to LangChain Agents (Automata Learning Lab) bit.ly/3R8x35h
---
ğ—£ğ—”ğ—¥ğ—§ ğŸ± - ğ—™ğ—œğ—¡ğ—˜ğ—§ğ—¨ğ—¡ğ—œğ—¡ğ—š (1h 30m)
Feature-based finetuning, LoRA, RLHF
Resources:
ğŸ“– 15m: Finetuning LLMs (Sebastian Raschka) bit.ly/4c0JckY
ğŸ“– 15m: Fine Tuning vs. Prompt Engineering LLMs (Niels Bantilan) bit.ly/3VlJWvi
ğŸ“º 30m: Reinforcement Learning from Human Feedback: From Zero to chatGPT (Hugging Face) bit.ly/453p1QV
ğŸ“– 15m: Complete Guide On Fine-Tuning LLMs using RLHF (Labellerr) bit.ly/3VnRA8K
ğŸ“– 15m: LoRA (Hugging Face) bit.ly/4aKfr6X