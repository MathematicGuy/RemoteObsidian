**Dự án: "Nghiên cứu và xây dựng mô-đun nhận diện hành vi của
sinh viên trong giảng đường"**

- **Mô tả:** Phát triển một hệ thống phân loại các hành vi bất thường đơn giản (ví dụ: chạy quá nhanh, leo trèo, tụ tập) trong video giám sát.
    
- **Dataset:**
    
    - **UCF-Crime:** Mặc dù dataset này chủ yếu về các hành vi tội phạm, một số hành vi (ví dụ: chạy, xô đẩy) có thể tương tự với hành vi bất thường trong trường học. Bạn có thể chọn lọc các video phù hợp.
        
    - **Real Life Violence Situations Dataset:** Dataset này có các video về hành vi bạo lực nhưng cũng có các hành vi khác như xô đẩy, tụ tập mà có thể sử dụng cho bài toán này.
        
    - **Tự thu thập:** Bạn có thể tự thu thập các video về các hành vi này từ YouTube hoặc các nguồn khác.
    
+ Note:
	Optimizer: PSO ([Particle swarm optimization](https://en.wikipedia.org/wiki/Particle_swarm_optimization))


---

**References:**
+ [LSTM](https://phamdinhkhanh.github.io/2019/04/22/Ly_thuyet_ve_mang_LSTM.html)
+ [Human Pose Detection using LSTM](https://nam157.github.io/human_activity_recognition-/)
+ [[Pose Estimation Note]]
+ [[Human Activity Recognition]]

---
### Video Format
`.avi`: 
`.mp4`



```python
"""
    Input: skeletons = {bodyi {j : (xjoint_j , yjoint_j ), j = [0, 17]}, i = [0, n −1]}
    boxes_object = {boxi (x1, y1, x2, y2 ), i = [0, m −1]}
    n: number of skeletons
    m: number of bounding boxes
    
    Output: boxes_skeleton = {boxi (x1, y1, x2, y2 ), i = [0, n −1]}
    boxes_all = {boxj (x1, y1, x2, y2 ), j = [0, k −1]}
    k: number of persons
"""

for body in skeletons:
    x1 = min(xbody )
    y1 = min(ybody )
    x2 = max(xbody )
    y2 = max(ybody )
    boxes_skeleton.append(x1, y1, x2, y2 )
    boxes_all = boxes_skeleton

    status_skeletons = [True for (i, item) in enumerate(skeletons)]

    for (index_box, box) in enumerate(boxes_object):
        index_max = -1
        number_joints = 0

        for (index_body, body) in enumerate(skeletons):
            if not(status_skeletons[index_body]):
                continue
            if number_joints < count_points(box, body):
                number_joints = count_points(box, body)
                index_max = index_body

    if index_max == -1:
        boxes_append(box)
    else:
        
        status_skeletons[index_max] = False
        boxes_skeleton[index_max] = box
        boxes_all[index_max] = box
return boxes_skeleton, boxes_all
```

---

**Requirements:** Kế hoạch thực hiện bám sát theo chủ đề đề tài NCKH. Có thể thiếu nhưng không thể thêm. $$N(y_{i}; b) = \frac{y_{i} - b}{(b_{w}, b_{h})}$$ where $y_{i}$ are the coordinate of the n joint (e.g. ankle)

*Solve Pose Estimation as Regression problems*

MSE (Loss)
![[Pasted image 20250115060102.png]]

**Pose Estimation as DNN-based Regressor** to detect and locating position of every single joints. 
![[Pasted image 20250115060225.png]]
-> Detect current pose base on joints locations. 
**Cascade of Pose Regressors** -> Learn features of the current pose
All we care about is the location of the joints.  

---
### Plan
[[Student Behavior Recognition System Planning]]
[[Human Pose Detection Planning (Behaviour Detection)]]

----
### MediaPipe (Low Computation + Highly Customizable)
**2 step Detector:** Detection + Tracking 
![[Pasted image 20250115163509.png]]

**Methods & Performances**
![[Pasted image 20250115170650.png]]

---
## Introduction 
1) Overview about the problem Problem + Movivation about the problem -> Historical Context of existing solutions and problems. 
2) Straight into my current approach, explain how it work intuitively (Use Mediapie to ... then using .... data as input for Deeplearning model like LSTM\GRU\DNN) -> Lead to current goal -> Benefits of this Method.
## Dataset and Implement Steps
**Dataset ->** Originally -> Problem Overfitting & Other Problem -> Modified the Dataset into ....

**Implement Step** (Focus on Training step, Rephrase Inference just like Training)
1) Introduce the workflow broadly (Top-Down)
2) Execute ideas Linearly, step by step (Bottom-Up)
	*2.1)* The training step begin from extracting videos of each Action from the Single_person_action dataset and preprocessed by resizing them to 1820x720 then use Mediapipe to extract skeletal data from the person inside the frame.   
	 
+ ? Each person skeletal data extract from a frame consist of 33 landmarks, each landmarks include 4 values that is (x, y, z, landmark confidence score) making a total of 132 features, these features can be a float or zero in case Mediapipe fail to detect any landmarks.  (Nói chi tiết hơn phần này)  ![[Mediapipe-Pose-landmarks.png | 500]]
	
	*2.2)* These skeletal data extracted from each videos in each action folder is then used to build the Dataset. Because LSTM and GRU take sequential input data and each video have different frame length. I define MAX_SEQ_LEN to make video's frame sequential, this mean I will have a nested list where for each video is a list contain multiple list (i.e. skeletal data extracted from the frame)  inside it .
	
	Landmarks with missing values (x, y or z) get padded with zero to make data sequence length remain fixed as input for LSTM and GRU

## Model and Evaluation Results


## Evaluation Results

