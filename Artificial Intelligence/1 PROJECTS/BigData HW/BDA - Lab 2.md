**Activate HBASE**
```sh
hbase shell
```

# Lab 2 Overview

## Step 1: Set Up Your HDP Sandbox

Before you begin, ensure:

- The **HDP Sandbox** is installed and running.
- You have **SSH access** to the HDP Sandbox.
- **HBase and Hive services** are running.

To check:
```bash
# SSH into the Sandbox
ssh root@<sandbox-ip>

# Start HBase if not running
start-hbase.sh

# Start Hive if needed
hive
```

---

## Step 2: Practice HBase

You'll execute HBase shell commands to create and manipulate a table.

### **1Ô∏è‚É£ Connect to HBase**

```bash
hbase shell
```

### **2Ô∏è‚É£ Create a Table**

```bash
create 'students', 'info'
```

### **3Ô∏è‚É£ Insert Data**

```bash
put 'students', '1001', 'info:name', 'Nam'
put 'students', '1001', 'info:age', '21'

put 'students', '1002', 'info:name', 'Thanh'
put 'students', '1002', 'info:age', '23'

put 'students', '1003', 'info:name', 'Hai5D5D5D'
put 'students', '1003', 'info:age', '22'
```
![[Pasted image 20250213091213.png]]
![[Pasted image 20250213091219.png]]

### **4Ô∏è‚É£ Retrieve Data**
```bash
get 'students', '1001'   # Get all details for student 1001
get 'students', '1002', 'info:name'  # Get only the name of student 1002
scan 'students'  # Retrieve all records
```
![[Pasted image 20250213083801.png]]

### **5Ô∏è‚É£ Modify Data**

```bash
put 'students', '1003', 'info:age', '22'  # Update Charlie‚Äôs age
```

### **6Ô∏è‚É£ Delete Data**
```bash
delete 'students', '1002', 'info:age'  # Remove Bob‚Äôs age
deleteall 'students', '1003'  # Delete Charlie‚Äôs row
```
![[Pasted image 20250213084219.png]]

### **7Ô∏è‚É£ Drop the Table**
```bash
disable 'students2'
drop 'students2'
```
Before
![[Pasted image 20250213091507.png]]
After
![[Pasted image 20250213091550.png]]

### **8Ô∏è‚É£ Add a New Column Family**
```bash
alter 'students', 'contact'
put 'students', '1001', 'contact:email', 'alice@example.com'
```
![[Pasted image 20250213084800.png]]

---

## Step 3: HBase and Pig Integration

You'll run a **Pig script** to load data into HBase.

### **1Ô∏è‚É£ Download Required Files**
```bash
wget https://raw.githubusercontent.com/ashaypatil11/hadoop/main/movies.user
wget https://raw.githubusercontent.com/ashaypatil11/hadoop/main/hbase_example.pig
```
Check if File Uploaded in Root File) 
+ ? `~` mean root file
```sh
ls ~
cd ~ # to to root file
```
	
### **2Ô∏è‚É£ Upload Files to HDFS**

```bash
hadoop fs -mkdir /user/maria_dev/pig
hadoop fs -copyFromLocal movies.user /user/maria_dev/pig/movies.user
```

### **3Ô∏è‚É£ Verify Pig Script**

```bash
less hbase_example.pig
```

### **4Ô∏è‚É£ Start HBase Services**

```bash
start-hbase.sh
```
if already start use
```sh
hbase shell
```

### **5Ô∏è‚É£ Execute Pig Script**

```bash
pig hbase_example.pig
```
![[Pasted image 20250217213937.png]]

check the **detailed error logs**:
`cat /var/log/pig/*.log | tail -50`
Access hadoop: `http://192.168.1.60:8088/cluster` 

### **6Ô∏è‚É£ Verify Data in HBase**
![[Pasted image 20250218065500.png]]
```bash
hbase shell
list
scan 'your_hbase_table_name'
```

---

## Step 4: Practice Hive

You'll work with **web store data** to perform SQL-like queries.

### **1Ô∏è‚É£ Download Sample Data**

Download and extract: [Click here](https://drive.google.com/file/d/1Otyp8MI5soC6qS1OeodHYuNkMbXFj6Tl/view?usp=sharing)

Extract:

- `Omniture.0.tsv.gz`
- `users.tsv.gz`
- `products.tsv.gz`

### **2Ô∏è‚É£ Upload Data to HDFS**

```bash
hadoop fs -mkdir /tmp/admin
hadoop fs -copyFromLocal Omniture.0.tsv /tmp/admin/
hadoop fs -copyFromLocal users.tsv /tmp/admin/
hadoop fs -copyFromLocal products.tsv /tmp/admin/
```

### **3Ô∏è‚É£ Create Hive Database**

```sql
CREATE DATABASE IF NOT EXISTS click;
```

### **4Ô∏è‚É£ Create Tables**

```sql
USE click;

CREATE TABLE users (
  swid STRING, 
  birth_dt STRING, 
  gender_cd CHAR(1)
) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE 
TBLPROPERTIES ("skip.header.line.count"="1");

CREATE TABLE products (
  url STRING, 
  category STRING
) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE 
TBLPROPERTIES ("skip.header.line.count"="1");

create table omniturelogs (col_1 STRING,col_2 STRING,col_3 STRING,col_4 STRING,
						   col_5 STRING,col_6 STRING,col_7 STRING,col_8 STRING,col_9 STRING,col_10 STRING,col_11 STRING,col_12 STRING,col_13 STRING,col_14 STRING,col_15 STRING,col_16 STRING,col_17 STRING,col_18 STRING,col_19 STRING,
						   col_20 STRING,col_21 STRING,col_22 STRING,col_23 STRING,col_24 STRING,
						   col_25 STRING,col_26 STRING,col_27 STRING,col_28 STRING,col_29 STRING,
						   col_30 STRING,col_31 STRING,col_32 STRING,col_33 STRING,col_34 STRING,
						   col_35 STRING,col_36 STRING,col_37 STRING,col_38 STRING,col_39 STRING,
						   col_40 STRING,col_41 STRING,col_42 STRING,col_43 STRING,col_44 STRING,
						   col_45 STRING,col_46 STRING,col_47 STRING,col_48 STRING,col_49 STRING,
						   col_50 STRING,col_51 STRING,col_52 STRING,col_53 STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED by '\t'
stored as textfile
tblproperties ("skip.header.line.count"="1");
```

### **5Ô∏è‚É£ Load Data into Hive**

```sql
LOAD DATA INPATH '/tmp/admin/products.tsv' OVERWRITE INTO TABLE products;
LOAD DATA INPATH '/tmp/admin/users.tsv' OVERWRITE INTO TABLE users;
LOAD DATA INPATH '/tmp/admin/Omniture.0.tsv' OVERWRITE INTO TABLE omniturelogs;
```

### **6Ô∏è‚É£ Run Queries**

```sql
SELECT * FROM users LIMIT 5;
SELECT * FROM products LIMIT 5;
SELECT COUNT(*) FROM omniturelogs;
```

---

## Step 5: Self-learning - Hortonworks ODBC Driver

Install the **Hortonworks ODBC Driver** to connect Hive with Power BI, Excel, or Tableau.

üîó **Guide**: [Watch Here](https://www.youtube.com/watch?v=PeTBqfUnwsw)

### Connect Hive with Python

```python
from pyhive import hive
import pandas as pd

# Establish connection
conn = hive.Connection(host='localhost', port=10000, username='hive')
cursor = conn.cursor()

# Execute a query
cursor.execute("SHOW TABLES")

# Fetch and print results
for table in cursor.fetchall():
    print(table)

# Close connection
cursor.close()
conn.close()
```

---

## **‚úÖ Summary**

| Task                                                        | Status |
| ----------------------------------------------------------- | ------ |
| ‚úÖ Set up HDP Sandbox                                        | ‚úÖ      |
| ‚úÖ Practice HBase (Create, Insert, Query, Delete)            | ‚úÖ      |
| ‚úÖ HBase-Pig Integration (Pig script for HBase)              | ‚úÖ      |
| ‚úÖ Practice Hive (Create tables, Load data, Query)           | ‚úÖ      |
| ‚úÖ Self-learning: Hortonworks ODBC & Python-Hive Integration | ‚úÖ      |
