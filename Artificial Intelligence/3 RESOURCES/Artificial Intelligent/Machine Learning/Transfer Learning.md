
## Transfer Learning
+ ? Instead of starting/training from scratch, model starting from some where (i.e. pre-trained model)
Example: We got a trained CNN model where the image go through the ConV layer to extract image's general features, apply pooling to extract out the best feaures out of the general features. As CNN layer goes deeper and deeper, its extract out the more abstract features which capture more inportant/specific information instead of generalize information in the first CNN layers.
![[Pasted image 20250226090441.png]]
'Transfer learning' tranfers features parameters/intelligent from 1 model to another model.  
![[Pasted image 20250226090750.png]]
### Transfer learning Strategy & Advantage
+ **Strategy**
	1) **Freeze the trained CNN network weights from the first layers** (make pre-train model un-trainable) so the pre-trained model parameters doesn't get adjust, basically destroy the original pre-train model
	2) Only train the newly added Dense Layers (with randonly initialized weights)
+ **Advantages**
	1) Provided fast training progress, you don't have to start from scratch.
	2) You can use small training dataset to achieve incredible result. (like when you understand the basic, you learn faster)
		![[Pasted image 20250226091452.png]]

## Transfer Learning Process
**TL process follow General Machine Learning Flow**
1. Examine and understand the data
2. Build an input pipeline, in this case using Keras ImageDataGenerator
3. Compose the model
    - Load in the pretrained base model (and pretrained weights)
    - Stack the classification layers on top
4. Train the model
5. Evaluate model

**Why Transfer Learning ?**
![[Pasted image 20250226083552.png]]
> Better Training Performance, Less Computational Cost. While pre-trained model transferring knowledge, its also transfer its bias.  

```python
# Create the base model from the pre-trained model MobileNet V2
IMG_SHAPE = IMG_SIZE + (3,)
base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')                                            
image_batch, label_batch = next(iter(train_dataset))
feature_batch = base_model(image_batch)
print(feature_batch.shape)
```
This feature extractor converts each `160x160x3` image into a `5x5x1280` block of features. Where 
+ `5x5` is the spatial dimension of the feature map. Like 5x5 grid where each cell inside its contains a feature vector.
+ `1280` is the features depth of each 5x5 grid (spatial location)

+ ? Freeze a model mean disable its trainning ability `setting layer.trainable = False`.
To transfer learning, we extract MobileNetV2 flattern layer or "bottleneck layer" which features retain more generality as compare to the final/top layer. 

As we set `trainable=False`, model only have 1,281 trainable params. The transfer training process are quite simple:
1) To generate predictions from the block of features, average over the spatial `5x5` spatial locations, using a `tf.keras.layers.GlobalAveragePooling2D` layer to convert the features to a single 1280-element vector per image.
	+ ? This layer **takes the high-dimensional feature maps** generated by previous layers and **compresses them into a lower-dimensional vector while retaining important information.**
```python
inputs = tf.keras.Input(shape=(160, 160, 3))
x = data_augmentation(inputs)
x = preprocess_input(x)

x = base_model(x, training=False)

x = global_average_layer(x)  
x = tf.keras.layers.Dropout(0.2)(x)
outputs = prediction_layer(x)
model = tf.keras.Model(inputs, outputs)
```
![[Pasted image 20250226085211.png]]



![[Pasted image 20250226081804.png]]
Reason that Validation Loss is clearly better than Train Loss is because of `BatchNormalization` and `Dropout` affect accuracy during training and they turned off then calculating validation loss.
-> Thus training metrics report the average for an epoch, while validation metrics are evaluate after the epoch so validation metrics so its see a model that been training slightly longer (i.e.  trainned model version). 

---
[ResNet Architecture](https://youtu.be/nc7FzLiB_AY?si=62M-gm4N3Rr_UTBa)
[Transfer Learning](https://www.youtube.com/watch?v=3gyeDlZqWko)

