## Lesson 1 - Optimization in Neural Network
### Regression with a Perceptron
alternate to gradiend descent called Newton's method (very fast and useful)

![[Pasted image 20240926150726.png]]
>**Main Goal:** Find weights and bias that will optimise the predictions -> optimal values for w1, w2 and b such that we minimize the error. 
>i.e. Reduce the error in the predictions (using The Loss Function) 

### Loss Function
>Find Loss by subtract the real value to the predicted value. Since this will cause the Loss both positvie and negative, we square Loss function and divided it by 2. (divide by 2 help cancel out the 2 when Loss function Derivative) 
![[Pasted image 20240926151419.png]]
![[Pasted image 20240926152102.png]]


Using Gradient Descent to find Loss Function, we first need to find the Partial Derivative of $\hat{y}$. Apply the chain rule, there are 3 partial derivative for each constant:
![[Pasted image 20240926152432.png]]
Let simplied this for what need to be calculated:
![[Pasted image 20240926153352.png]]
![[Pasted image 20240926153335.png]]
Let update these Partial Derivative to find w1, w2 and b
![[Pasted image 20240926153510.png]] 

### Classification with Perceptron
#### Motivation: classified output base on input features
![[Pasted image 20240927085659.png]]

We have a word table contain 2 unique word with their repeated time. Since our models take numerical input, we can plot  the point on 2 features axis: Aack and Beep, their repeated times as value.
>Now we need a function represent the line which divide the plot if emotion are happy or sad (0 & 1)
![[Pasted image 20240927090004.png]]

>Our perceptron will have 2 nodes: W_1 and W_2 determine how important any other features are, say the word `aack` is really **correlated with happiness** then **W_1 going to be a larger number**, if `beep` is really **irrelevant (ko li√™n quan)** then **W2 is going to be a small number**.
+ ? **How can we turn entire number line 0 and 1**, we're going to use what's called the activation function, denoted by the letter **Sigmoid of z**
![[Pasted image 20240927085639.png]]


#### Classification with Perceptron - Sigmoid Function
input - x
output - y
![[Pasted image 20240927092127.png]]

Sigmoid function crunches the entire number line into the interval (0, 1) and is that its derivative is really nice.
+ ? Fomula use in the derivative below is: $e^u$, the chain rule, basic derivatives 
 ![[Pasted image 20240927092320.png]]
 ![[Pasted image 20240927092450.png]]
 ![[Pasted image 20240927092622.png]]


#### Classification with Perceptron - Gradient Descent
in normal regression problem we use least square error to calc the Loss. For **Classification problem**, the **best function is the Log Loss**. 
![[Pasted image 20240927100240.png]]
$L(y, \hat{y})$ 
	![[Pasted image 20240927101436.png]]
+ Larger if $\hat{y}$ is close to 1, Smaller if $\hat{y}$ is close to 0.
+ Larger number if $y$ and $\hat{y}$ is far from each other, and small if $y$ and $\hat{y}$ are close to each other. 

Like before, we find w1, w2 and b such that $\hat{y}$ give the least errors using Gradient Descent. To achieve that, we first find Derivative of the Loss function in respect of $w1,\space w2,\space b$. Then start with initial value: w1, w2, b to activate Gradient Descent. Finally put (w1, w2 and b) or $\hat{y}$ back the Loss Function to verify.     
![[Pasted image 20240927101754.png]]
> Because $\hat{y}$ function is inside $L(\hat{y}, y)$ function, we use the Chain Rule to seperate each derivate for simplification and tackle each of them 1 by 1. 
	![[Pasted image 20240927112155.png]]
	![[Pasted image 20240927112132.png]]
	For $\hat{y}$, we already have the derivative of $\sigma$ calculated as $\sigma' = \sigma(z).(1 - \sigma(z))$, using the chain rule we have: $\sigma'.(w_{1}x_{1} + w_{2}x_{2} + b)$ with respect of w1, w2 and b.
	![[Pasted image 20240927113039.png]]
	Put each derivative back, we have most of the denominator and end up with this very simplified partial derivative in respect of w1, w2, and b 
	![[Pasted image 20240927113434.png]]
	![[Pasted image 20240927113542.png]]
>Let put these back to ours Gradient Descent function:
>![[Pasted image 20240927113657.png]]

### Classification Problem Motivation
+ ? **Neural Network**: bunch of perceptron stacked in layers.
+ ? **How to train theses neural networks?** just using gradient descent like before. Except now we have many derivatives.  

Let build a neural network to classify emotion.
![[Pasted image 20240930161130.png]]

**2,2,1 Neural network**   
![[Pasted image 20240930161010.png]]
+ ? Each node have the previous layer weight and bias. (both hidden and output) and let not forget the Log Loss function we calc. 
![[Pasted image 20240930162839.png]]

+ $ **Goals:** Adjust each of the weights and biases to reduce the loss function.
+ ? How do these biases affect the loss? with partial derivative. In other words, this partial derivative tell us exactly what direction to move each one of the weights and biases in order to reduce the log loss function. ![[Pasted image 20240930163826.png]]

Like before, we going to use the chain rule so let look at the derivative of each weight as bias first:
![[Pasted image 20240930163948.png]]

So there a lot of variables in between L and W11. And we're going to keep track all of them by a humongous chain rule. The reason is because the loss function $L(y, \hat{y})$  depend on $\hat{y}$ so we need $\frac{\Delta L}{\Delta\hat{y}}$, and $\hat{y}$ depend on $z$ so we need $\frac{\Delta \hat{y}}{\Delta z}$ and $a$ depend on $z$ so we have $\frac{\Delta z}{\Delta a_{1}}$ and z1 is depend on a1 so we have $\frac{\Delta a_{1}}{\Delta z_{1}}$ and w11 is depend on z1 so we have $\frac{\Delta z_{1}}{\Delta w_{11}}$. This NN is the combination of all these derivative.
![[Pasted image 20240930164241.png]]

Calculating all the derivative have the $\frac{\Delta L}{\Delta\hat{y}}$ Log Loss function of this 2,2,1 Neural Network as below:
![[Pasted image 20240930165805.png]]
Replace Loss function to the Gradient Descent
$$w_{11} \to w_{11} - \alpha.(-x_{1}.w_{1}.a_{1}.(1-a_{1})(y-\hat{y}))$$

+ ? The Chain Rule Process repeat for every weights and biases. i.e. $w_{11}, w_{12}, b, etc..$ (yes, even the bias)
	![[Pasted image 20240930170521.png]]
	![[Pasted image 20240930170540.png]]

**Summary**: If you do these updates for a learning rate $\alpha$, you get better weights and biases. (That is for the first layer) 
![[Pasted image 20240930170622.png]]

**Second Layer**
![[Pasted image 20240930171055.png]]

**w1, w2 and b**
![[Pasted image 20240930171122.png]]
![[Pasted image 20240930171129.png]]

### Gradient Descent and Backpropagation
> Backward Derivative (use Chain Rule)

Node notation (k√Ω hi·ªáu) in each layer.  
![[Pasted image 20241001095227.png]]

Eveything can be calculated using these weights (notate in green). In Backpropagation we calc all the derivative in backward.
	![[Pasted image 20241001095645.png]]
Let Begin with $\frac{\Delta L}{\Delta w^{[3]}}$
	![[Pasted image 20241001095808.png]]

+ ? The derivative at the bottom right is already calculated so remember to store them for reused.
![[Pasted image 20241001100311.png]]
+ ? For the last one, we just have to calc the node itself, bc its previous node are already calcualated (in bottom right)
![[Pasted image 20241001100411.png]]


## Lesson 2: Newton's method
> Newton's Method is used to find the 0 of a function.

Let's take the tangent at this point X0, so let's take the derivative and draw the tangent and see where it hits the horizontal X axis. And at this point we're going to call it X1, notice that X1 is much closer to zero than X0. We draw more time for X2, its very close to 0, do this one more time X3 pretty much found it (very close)
![[Pasted image 20241001101700.png]]

The Slope is f(X0), it base is X0 - X1. The slope is calc by Rise over Run. So $$f'(x_{0}) = \frac{f(x_{0})}{x_{0} - x_{1}}$$ Simplified it we get $x_{1}$
![[Pasted image 20241001102053.png]]
$$x_{k+1} = x_{k} - \frac{f(x_{0})}{f'(x_{0})}$$
and got $x_{k}, x_{k-1}$ as the first 2 points created the slope. Iterate on the step we get Newton's method.    
![[Pasted image 20241001102742.png]]

So how Newton's Method can be use for optimization. 
+ ? Gradient Decent found the minimum of a function, Newton's method only find the 0 of the function not the minimum (e.g. f(x) = 0). Well, to minimize g(x) we have to find zeros of $g'(x)$ so we just need to chose which zeros is the minimum. (cho ptr b·∫±ng 0 v√† ch·ªçn nghi·ªám b√© nh·∫•t) 
+ $ In other word, if I let F of X be the derivative of G of X, then by finding the zeros of F of X, I am minimizing G prime of X. And the derivative of F prime of X, is simply the derivative of G of X and the derivative of that.

In NM, we first initiate $x_0$ as the starting point and update it. For Optimization, because by finding the derivative of $f(x_{k})$ also minimizing $g(x)'$. We replace F of X with G prime of X.
	![[Pasted image 20241001103540.png]]![[Pasted image 20241001103933.png]]


## Newton's Method Example
Back to e hat x minus log of x example. g'(x) =0.5671 (given answer)
Let see if Newton's Method work. we have G prime of x therefor ours function for f prime becomed $(g'(x))'$ since f(x) is depend on g(x). Starting at $x_{0}=0.05$ we iterate until reached the minimum: 0.5671.  
![[Pasted image 20241001104348.png]]
After 5 iterations we found it. Quite like Gradient Descent right:
![[Pasted image 20241001105555.png]]

### Second Derivative
> Rate of change of "the rate of change"

note:
+ derivative of a func is a constance mean that func is a line.
+ curvature: ƒë·ªô cong
+ convex: l·ªìi

**Notation**
![[Pasted image 20241001112719.png]]

Understanding Second Derivative with Real Life example
![[Pasted image 20241001112943.png]]
The map show the change in distance x through time t.  
+ ? For each point of time we map the line function. And calculate the derivative with respect of t. 
+ ? Lines in the 1st, 2nd Derivative because all functions result are constants. 

>The first derivitive show the car's Speed
![[Pasted image 20241001114056.png]]

>  The second derivative show the car's Acceleration
![[Pasted image 20241001113239.png]]
```ad-summary
The 2nd Derivative result positive when the car accelerating and result negative when the car deaccelerating. When the car speed is constant (speed unchanged) 2nd Derivative result in 0.
```


**Curvature**
note: 2D mean 2nd Derivative
![[Pasted image 20241001114951.png]]
(h∆∞·ªõng l√™n khi 2D > 0. h∆∞·ªõng xu·ªëng khi 2D < 0, c·∫ßn th√™m th√¥ng tin khi 2D = 0)

![[Pasted image 20241001115148.png]]
+ ? As the image showed, local min when 2D>0 and local max when 2D<0.
Note: Inconclusive (ko c√≥ k·∫øt lu·∫≠n) mean we don't know it maximum or minimum.

[[1st Derivative vs 2nd Derivative]]
![[Pasted image 20241001115337.png]]
> The 2nd Derivative tell at how the "rate of change" (1st Derivative) is changing.

```ad-summary
- The **first derivative** gives the **rate of change** or **slope** of the function.
- The **second derivative** gives information about the **concavity** or **curvature**.
- For a **quadratic function**, the **second derivative is constant**, which gives rise to a **parabolic shape**.
- Higher-order functions have **changing second derivatives**, resulting in more complex and varied curves, not just parabolas.
```


## The Hessian (Matrix)
+ $ Example above was just 1 variable, for multiple variables, the **second derivative is a Matrix full of second derivatives called the Hessian**.

![[Pasted image 20241001142935.png]]

Explain from the top down, we got the rate of change of $f_{x x}$ (this is just $f'(f'(x))$ 
	the same for the rest, $f_{xy}$, $f_{yy}$, $f_{yx}$  (i.e. rate of change of the derivative in respect of ...)
+ ? **w.r.t** mean with respect to 
	![[Pasted image 20241001143458.png]]
	![[Pasted image 20241001143445.png]]
+ orthogonal: tr·ª±c giao. 
	![[Pasted image 20241001143512.png]]
+ change in the change: derivative of the derivative i.e. rate of change of "the rate of change".

note: $f_{x x}$ just mean derivative 2 times. 
![[Pasted image 20241001143534.png]]

took the derivative in respect of x and y.
![[Pasted image 20241001143630.png]]
another example:
![[Pasted image 20241001143745.png]]
**Notation**
> **Hessian matrix is a matrix that keep track of all the 2nd Derivatives of a function.**
> -> This helps us optimize functions with multiple variables. This also called multivariable Newton's method.
![[Pasted image 20241001144237.png]]
+ ? Bonus Fact: **By analyzing the Hessian matrix**, we **can determine whether a point is a minimum, maximum, or a saddle point**
![[Pasted image 20241001144722.png]]


### Hessians and concavity
Example:
+ At (0, 0) both eigenvalue are positive: the function is concave up and the 0.0 is minimum.
	![[Pasted image 20241001144955.png]]
	
+ Concabe Down maximum at (0,0) because eigen value is < 0.
	![[Pasted image 20241001145156.png]]
	
+ Saddle Point (look like a saddle, **a place between maximum and minimum as you can see**): Situation when both eigenvalue is < 0 and > 0. This cannot be determinant, we call it saddle point.
	![[Pasted image 20241001145326.png]]

**In Summary:**
+ If all variable are < 0: the function is concave down and maximum at point (x, y)
+ If all variable are  > 0: the function is concave up and minimum at point (x, y)
+ If any variable = 0 or < 0 and > 0 at the same time -> saddle point thus cannot be determinant.
	![[Pasted image 20241001145524.png]]

### Newton's Method for 2 variables.
> Learn how to apply Newton's method to optimize functions of many variables. And as you guessed it, the Hessian appears here.

+ 1 var: the func and it re-write version.
+ 2 vars or n vars: hessian matrix version (newton's method in matrix version)
![[Pasted image 20241001145924.png]]
+ ? **Notation:** 
	+ $\nabla f$: the gradient (as the 1st derivative with respect of x and y)
	+ $H^{-1}$:  The hassien matrix (as the 2nd derivative with respect of x and y)
+ ! remember, you cannot multiply 2 by 1 vector to the left of the matrix. (i.e. Multiply $\nabla f$ with $H^{-1}$). The oder is crucial !!!
	![[Pasted image 20241001150146.png]]
+ the 2 vars fomula is quite comflex so now we not prove it yet. Accept it as it is for now.

Example:
![[Pasted image 20241001150400.png]]

![[Pasted image 20241001150513.png]]
![[Pasted image 20241001150540.png]]
now let Iterate again, and repeat it until we reach the minimum point (very close to the actual 0).
![[Pasted image 20241001150558.png]]
![[Pasted image 20241001150646.png]]
> This is incredibly fast for finding the minimum point of higher dimension function.


### Gradient Descent vs Newton's Method in calc Multiple Variables

**Gradient Descent:** have learning rate as an extra parameter and converge slower than Newton's Method.  (derivative 1 time at a row). Might not even converge it the dimension is too large.

**Newton's Method:** Converge fast but each variable need dervative 3 times made it computationally expensive that is $n + 2n$ (i.e. 3 variables need derivative 9 times: it self (3 times), its derivative derivative with respect of x and y (each derivative derivate 2 times: 3.2=6 times total))
	![[Pasted image 20241001162406.png]]
+ $ This is the reality of numerical optimization - convergency and actual result depend on the initial parameters. Also, there is no "perfect" algorithm - every method will have advantages and disadvantages.
	
+ reciprocal: ngh·ªãch ƒë·∫£o
	**The reciprocal of a non-zero real number** $a$ **is** $\frac{1}{a}$

![[Pasted image 20241001170233.png]]![[Pasted image 20241001170219.png]]

## Optimization in Neural Network and Newton's Method 
## 1 - Classification Problem
![[Pasted image 20241002153710.png]]
+ ? Each layer distinguish by the integer called superscript (i.e. number above) in the square brackets $^{[1]}$ denotes the layer number. 
+ ? nx as the number of input for input layer, nh as the number of nodes in hidden layer, ny as the number of nodes in output layer. 

Simplified the first 2 parameters:
The training examples $x^{(i)}=\begin{bmatrix}x_1^{(i)} \\ x_2^{(i)}\end{bmatrix}$ from the input layer of size $n_x = 2$ are first fed into the hidden layer of size $n_h = 2$ along with b (i.e. scaler):
![[Pasted image 20241002154045.png]]
rewritten in matrix form.
+ ? z hat 1 i represent each vector in layer 1. 
+ ? W hat 1 represent the weight of each node in layer 1. W hat 1 1 mean weights from layer 1 node 1 which contain: $w_{1,1}$ and $w_{2,1}$ the same for w hat 1 2. 
+ ? b hat 1 mean b in the 1st layer, b hat 1 1 mean b of the first node and b hat 1 2 mean b of the 2nd node (both in the 1st layer)

+ $ These **expressions for one training example** $x^{(i)}$ can be rewritten in a matrix form :
$$z^{[1](i)} = W^{[1]} x^{(i)} + b^{[1]},\tag{2}$$
![[Pasted image 20241002153757.png]]


We use sigmoid as ours activation function $\sigma\left(x\right) = \frac{1}{1 + e^{-x}}$: Rewritten in matrix form we get: 
$$a^{[1](i)} = \sigma\left(z^{[1](i)}\right) = 
\begin{bmatrix}\sigma\left(z_1^{[1](i)}\right) \\ \sigma\left(z_2^{[1](i)}\right)\end{bmatrix}.\tag{3}$$
Then the hidden layer output gets fed into the output layer of size $n_y = 1$. This was covered in the previous lab, the only difference are: $a^{[1](i)}$ is taken instead of $x^{(i)}$ (i.e. x from input layer) and layer notation $^{[2]}$ appears to identify all parameters and outputs:
$$z^{[2](i)} = w_1^{[2]} a_1^{[1](i)} + w_2^{[2]} a_2^{[1](i)} + b^{[2]}= W^{[2]} a^{[1](i)} + b^{[2]},\tag{4}$$
rewrite all the expression, group them for convenience.
![[Pasted image 20241002162221.png]]
Finally, the predictions for some example $x^{(i)}$ (i.e. input $x^{(i)}$) can be made taking the output $a^{[2](i)}$ and calculating $\hat{y}$ as: $\hat{y} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5, \\ 0 & \mbox{otherwise }. \end{cases}$


### 2.2 - Neural Network Model with Two Layers for Multiple Training examples
+ $ **Goals:** perform training of the model, find such set of parameters $ùëä^{[1]}$, $ùëè^{[1]}$ (i.e. layer 1), $ùëä^{[2]}$, $ùëè^{[2]}$ (i.e. layer 2), that will minimize the cost function.
+ ? **True Label:** Correct output $y^{(i)}$ of the function. (either 0 and 1)
+ ? **Test Label:**  Label use to test and compare with true label.
+ note: $\hat{y}$ now replace as $a^{[2](i)}$
	Like in the previous example of a single perceptron neural network, the cost function can be written as:
$$\mathcal{L}\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\right) = \frac{1}{m}\sum_{i=1}^{m} L\left(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}\right)$$$$=  \frac{1}{m}\sum_{i=1}^{m}  \large\left(\small - y^{(i)}\log\left(a^{[2](i)}\right) - (1-y^{(i)})\log\left(1- a^{[2](i)}\right)  \large  \right), \small\tag{8}$$where $y^{(i)} \in \{0,1\}$ are the original labels and $a^{[2](i)}$ are the continuous output values of the forward propagation step (elements of array $A^{[2]}$).
+ $ **Loss Function Purpose**: Measures the discrepency (kh√°c bi·ªát, t∆∞∆°ng ph·∫£n) between predicted probability and the true label. 

To minimize loss we updating the parameters using gradient descent:
$$\begin{align}
W^{[1]} &= W^{[1]} - \alpha \frac{\partial \mathcal{L} }{ \partial W^{[1]} },\\
b^{[1]} &= b^{[1]} - \alpha \frac{\partial \mathcal{L} }{ \partial b^{[1]} },\\
W^{[2]} &= W^{[2]} - \alpha \frac{\partial \mathcal{L} }{ \partial W^{[2]} },\\
b^{[2]} &= b^{[2]} - \alpha \frac{\partial \mathcal{L} }{ \partial b^{[2]} },\\
\tag{9}
\end{align}
$$where $\alpha$ is the learning rate.

To perform training of the model you need to calculate now $\frac{\partial \mathcal{L} }{ \partial W^{[1]}}$, $\frac{\partial \mathcal{L} }{ \partial b^{[1]}}$, $\frac{\partial \mathcal{L} }{ \partial W^{[2]}}$, $\frac{\partial \mathcal{L} }{ \partial b^{[2]}}$ (i.e. Derivative of Weight and bias from 1st and 2nd layer annotation in $^{[1], \space [2]}$)  

Let's start from the end of the neural network. You can rewrite here the corresponding expressions for $\frac{\partial \mathcal{L} }{ \partial W }$ and $\frac{\partial \mathcal{L} }{ \partial b }$ from the single perceptron neural network:
$$\begin{align}
\frac{\partial \mathcal{L} }{ \partial W } &= 
\frac{1}{m}\left(A-Y\right)X^T,\\
\frac{\partial \mathcal{L} }{ \partial b } &= 
\frac{1}{m}\left(A-Y\right)\mathbf{1},\\
\end{align}$$
where $\mathbf{1}$ is just a ($m \times 1$) vector of ones. Your one perceptron is in the second layer now, so $W$ will be exchanged with $W^{[2]}$, $b$ with $b^{[2]}$, $A$ with $A^{[2]}$, $X$ with $A^{[1]}$:
$$\begin{align}
\frac{\partial \mathcal{L} }{ \partial W^{[2]} } &= 
\frac{1}{m}\left(A^{[2]}-Y\right)\left(A^{[1]}\right)^T,\\
\frac{\partial \mathcal{L} }{ \partial b^{[2]} } &= 
\frac{1}{m}\left(A^{[2]}-Y\right)\mathbf{1}.\\
\tag{10}
\end{align}$$

Let's now find $$\frac{\partial \mathcal{L} }{ \partial W^{[1]}} = 
\begin{bmatrix}
\frac{\partial \mathcal{L} }{ \partial w_{1,1}^{[1]}} & \frac{\partial \mathcal{L} }{ \partial w_{2,1}^{[1]}} \\
\frac{\partial \mathcal{L} }{ \partial w_{1,2}^{[1]}} & \frac{\partial \mathcal{L} }{ \partial w_{2,2}^{[1]}} \end{bmatrix}$$It was shown in the videos that $$\frac{\partial \mathcal{L} }{ \partial w_{1,1}^{[1]}}=\frac{1}{m}\sum_{i=1}^{m} \left( 
\left(a^{[2](i)} - y^{(i)}\right) 
w_1^{[2]} 
\left(a_1^{[1](i)}\left(1-a_1^{[1](i)}\right)\right)
x_1^{(i)}\right)\tag{11}$$
If you do this accurately for each of the elements $\frac{\partial \mathcal{L} }{ \partial W^{[1]}}$, you will get the following matrix:

$$\frac{\partial \mathcal{L} }{ \partial W^{[1]}} = \begin{bmatrix}
\frac{\partial \mathcal{L} }{ \partial w_{1,1}^{[1]}} & \frac{\partial \mathcal{L} }{ \partial w_{2,1}^{[1]}} \\
\frac{\partial \mathcal{L} }{ \partial w_{1,2}^{[1]}} & \frac{\partial \mathcal{L} }{ \partial w_{2,2}^{[1]}} \end{bmatrix}$$
$$= \frac{1}{m}\begin{bmatrix}
\sum_{i=1}^{m} \left( \left(a^{[2](i)} - y^{(i)}\right) w_1^{[2]} \left(a_1^{[1](i)}\left(1-a_1^{[1](i)}\right)\right)
x_1^{(i)}\right) & 
\sum_{i=1}^{m} \left( \left(a^{[2](i)} - y^{(i)}\right) w_1^{[2]} \left(a_1^{[1](i)}\left(1-a_1^{[1](i)}\right)\right)
x_2^{(i)}\right)  \\
\sum_{i=1}^{m} \left( \left(a^{[2](i)} - y^{(i)}\right) w_2^{[2]} \left(a_2^{[1](i)}\left(1-a_2^{[1](i)}\right)\right)
x_1^{(i)}\right) & 
\sum_{i=1}^{m} \left( \left(a^{[2](i)} - y^{(i)}\right) w_2^{[2]} \left(a_2^{[1](i)}\left(1-a_2^{[1](i)}\right)\right)
x_2^{(i)}\right)\end{bmatrix}\tag{12}$$
+ $  We called this the Loss Function of each parameters (i.e. weight ans biases). Basically, normally what we do is calculate the loss of a weight and biases of each node by the derivative of it and $\mathcal{L}$ using the chain rule: $\frac{\partial\mathcal{L}}{\partial W_{11}}$. This does just that but in a greater scale which calc the weight and bias of 2 input with 2 nodes (in hidden layer). 
	reference:![[Pasted image 20241003101310.png]]


note: understand how they compress chain rule into a loss function
```python
np.log([1, np.e, np.e**2, 0])
```

```python
def layer_sizes(X, Y):
    """
    Arguments:
    X -- input dataset of shape (input size, number of examples)
    Y -- labels of shape (output size, number of examples)
    
    Returns:
    n_x -- the size of the input layer
    n_h -- the size of the hidden layer
    n_y -- the size of the output layer
    """


def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    params -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """


def forward_propagation(X, parameters):
    """
    Argument:
    X -- input data of size (n_x, m)
    parameters -- python dictionary containing your parameters (output of initialization function)
    
    Returns:
    A2 -- the sigmoid output of the second activation
    cache -- python dictionary containing Z1, A1, Z2, A2 
    (that simplifies the calculations in the back propagation step)
    """


def compute_cost(A2, Y):
    """
    Computes the cost function as a log loss
    
    Arguments:
    A2 -- The output of the neural network of shape (1, number of examples)
    Y -- "true" labels vector of shape (1, number of examples)
    
    Returns:
    cost -- log loss
    
    """


def backward_propagation(parameters, cache, X, Y):
    """
    Implements the backward propagation, calculating gradients
    
    Arguments:
    parameters -- python dictionary containing our parameters 
    cache -- python dictionary containing Z1, A1, Z2, A2
    X -- input data of shape (n_x, number of examples)
    Y -- "true" labels vector of shape (n_y, number of examples)
    
    Returns:
    grads -- python dictionary containing gradients with respect to different parameters
    """


def update_parameters(parameters, grads, learning_rate=1.2):
    """
    Updates parameters using the gradient descent update rule
    
    Arguments:
    parameters -- python dictionary containing parameters 
    grads -- python dictionary containing gradients
    learning_rate -- learning rate for gradient descent
    
    Returns:
    parameters -- python dictionary containing updated parameters 
    """


```

---
### M·∫°ng neuro 2 l·ªõp cho ƒë·∫ßu v√†o 1 gi√° tr·ªã (vd: $x_{1}=2, x_{2}=3$)
![[Pasted image 20241011103012.png]]
c√≥ ![[Pasted image 20241011103505.png]] ($x^{i}$ l√† vector ch·ª©a c√°c gi√° tr·ªã ƒë·∫ßu v√†o) l√† l·ªõp ƒë·∫ßu v√†o c√≥ k√≠ch th∆∞·ªõc ![[Pasted image 20241011103639.png]] ƒë∆∞·ª£c ƒë·∫©y v√†o l·ªõp ·∫©n k√≠ch th∆∞·ªõc ![[Pasted image 20241011103631.png]]
. Ch√∫ng ƒëc ƒë·∫©y v√†o l·ªõp Perceptron $z_{1}$ v·ªõi ![[Pasted image 20241011103813.png]] v√† $z_{2}$![[Pasted image 20241011103854.png]]. k√Ω hi·ªáu s·ªë v·ªõi ngo·∫∑c vu√¥ng tr√™n c√°c k√Ω hi·ªáu ƒë·∫°i di·ªán cho s·ªë l·ªõp hi·ªán t·∫°i (vd: 1 nghƒ©a l√† l·ªõp 1)
Nh√¢n c√°c tham s·ªë v·ªõi nhau v√† c·ªông th√™n bias ta c√≥:p 
![[Pasted image 20241011104023.png]]
v·ªõi
![[Pasted image 20241011104045.png]]
z l√† tham s·ªë ƒë·∫ßu v√†o c√≥ k√≠ch th∆∞·ªõc $n_{h} *1$ c·ªßa h√†m k√≠ch ho·∫°t a
W l√† ma tr·∫≠n c·ªßa weight c√≤n b l√† vector c·ªßa bias.

Ti·∫øp theo, h√†m k√≠ch ho·∫°t trong l·ªõp ·∫©n c·ªßa tham s·ªë $a$  c·∫ßn ƒë∆∞·ª£c √°p d·ª•ng cho m·ªçi ph·∫ßn t·ª≠ vector z.
![[Pasted image 20241011104724.png]]
H√†m k√≠ch ho·∫°t sigmoid: $\sigma\left(x\right) = \frac{1}{1 + e^{-x}}$. c√≥ ƒë·∫°o h√†m l√† $\frac{d\sigma}{dx} = \sigma\left(x\right)\left(1-\sigma\left(x\right)\right)$. V√¨ H√†m k√≠ch ho·∫°t nh·∫≠n vector z l√†m tham s·ªë n√™n a bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng:
![[Pasted image 20241011104411.png]]

R·ªìi ƒë·∫ßu ra c·ªßa l·ªõp ·∫©n ƒë∆∞·ª£c ƒë∆∞a v√†o "l·ªõp ƒë·∫ßu ra" c√≥ k√≠ch th∆∞·ªõc $n_{y}=1$.    
![[Pasted image 20241011104709.png]]
Do z nh·∫≠n 2 tham s·ªë $w_{1}, w_{2}$ n√™n ta c√≥ ph∆∞∆°ng tr√¨nh c·ªßa $z_{2}$ l√†:
![[Pasted image 20241011105020.png]]
note: W l·ªõn t∆∞·ª£ng tr∆∞ng cho m·ªçi w c·ªßa l·ªõp ·∫©n, v√† a t∆∞·ª£ng tr∆∞ng cho m·ªçi a c·ªßa l·ªõp ·∫©n.
+ Sau khi ƒëi qua h√†m sigmoid, z tr·ªü thanh vector v√¥ h∆∞·ªõng (1x1). V·∫≠y l√† z v√† b ƒë·ªÅu l√† vecter v√¥ h∆∞·ªõng (1x1) khi truy·ªÅn t·ªõi l·ªõp ƒë·∫ßu ra.
+ Ch·ªâ c√≤n W ƒë·∫°i di·ªán cho tham s·ªë $w1, w2$ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector:
![[Pasted image 20241011105456.png]]

Sau khi t√≠nh z, h√†m sigmoid ƒë∆∞·ª£c s·ª≠ d·ª•ng cho l·ªõp ·∫©n v√† truy·ªÅn t·ªõi h√†m Loss.
![[Pasted image 20241011105553.png]]

C√°c c√¥ng th·ª©c c·ªßa m·∫°ng neuron ph√≠a tr√™n c√≥ th·ªÉ vi·∫øt l·∫°i ng·∫Øn g·ªçn nh∆∞ sau:
$$
\begin{align}
z^{[1](i)} &= W^{[1]} x^{(i)} + b^{[1]},\\
a^{[1](i)} &= \sigma\left(z^{[1](i)}\right),\\
z^{[2](i)} &= W^{[2]} a^{[1](i)} + b^{[2]},\\
a^{[2](i)} &= \sigma\left(z^{[2](i)}\right).\\
\tag{6} \\
\end{align}
$$
v·ªõi $i$ ƒë·∫°i di·ªán cho c√°c bi·∫øn ƒë·∫ßu v√†o (th·ª© t·ª± c·ªßa c√°c bi·∫øn ƒë·∫ßu v√†o). vd: $z^{[1](i)}$ nghƒ©a l√† m·ªçi z l·ªõp 1.

sau ƒë√≥ ¬†$x^{(i)}$ c√≥ th·ªÉ ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch l·∫•y ƒë·∫ßu ra $a^{[2](i)}$ v√† t√≠nh $\hat{y}$ nh∆∞ sau: $\hat{y} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5, \\ 0 & \mbox{otherwise }. \end{cases}$.


## M·∫°ng Neuron 2 L·ªõp cho ƒë·∫ßu v√†o nhi·ªÅu gi√° tr·ªã (vd: $x_{1}=[1,2,3]$) 
![[Pasted image 20241011103012.png]]
note: ƒê·∫ßu v√†o l√† x, x ƒë∆∞·ª£c truy·ªÅn v√†o l·ªõp ·∫©n ƒë·ªÉ t√≠nh z, z truy·ªÅn t·ªõi h√†m k√≠ch ho·∫°t ƒë·ªÉ t√≠nh a. V√† output ra a t·ªõi l·ªõp ti·∫øp theo. (L·ªõp ·∫®n: ƒê·∫ßu v√†o x, ƒê·∫ßu ra a) ([[What is X]])

G·ªçi s·ªë l·∫ßn hu·∫•n luy·ªán l√† m (ƒë·ªÉ bi·ªÉu di·ªÖn ph∆∞∆°ng tr√¨nh t·ªët h∆°n), m c√≥ th·ªÉ ƒë∆∞·ª£c s·∫Øp x·∫øp trong ma tr·∫≠n X c√≥ k√≠ch th∆∞·ªõc (2 √ó m) (g·ªìm m c·ªôt) (V√¨ 2 bi·∫øn X ƒë∆∞·ª£c hu·∫•n luy·ªán m l·∫ßn).
+ ? vd X khi s·ªë ƒë·∫ßu v√†o $n_{x}=3$ c√≥ $m=4$. Khi ƒë√≥ m·ªói bi·∫øn l√† 1 bi·ªÉu di·ªÖn vector $x_{ij}$ c√≥ $i$ l√† th·ª© t·ª± c·ªßa vector, v√† $j$ l√† th·ª© t·ª± c√°c gi√° tr·ªã c·ªßa vector $i$.  
	![[Pasted image 20241011112646.png]]

V·ªõi ma tr·∫≠n X, ta ƒë∆°n gi·∫£n thay X l·ªõn cho x nh·ªè. Th·ªÉ hi·ªán r·∫±ng X l√† 1 ma tr·∫≠n ch·ª©a nhi·ªÅu vector $x_{i}$
![[Pasted image 20241011110846.png]]

ƒê·ªÉ ƒë√°nh gi√° m·∫°ng neuron m√¨nh khai b√°o h√†m Log Loss (cho b√†i to√°n ph√¢n lo·∫°i)
![[Pasted image 20241011121201.png]]
> "V√¨ h√†m m·∫•t m√°t $\mathcal{L}$ ph·ª• thu·ªôc v√†o $\hat{y}$ (k·∫øt qu·∫£ d·ª± ƒëo√°n), m√† $\hat{y}$ ƒë∆∞·ª£c t√≠nh th√¥ng qua c√°c tr·ªçng s·ªë v√† ƒë·ªô l·ªách (i.e. $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$) c·ªßa m√¥ h√¨nh, n√™n h√†m Log Loss th·ª±c ch·∫•t ph·ª• thu·ªôc v√†o c√°c tham s·ªë ·ªü l·ªõp ƒë·∫ßu v√†o v√† l·ªõp ·∫©n."
>.
>"ƒê·ªÉ ƒë√°nh gi√° m·∫•t m√°t c·ªßa m√¥ h√¨nh tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu $X$, ta t√≠nh t·ªïng gi√° tr·ªã m·∫•t m√°t cho m·ªói v√≠ d·ª• (i.e. vector c·ªßa X) v√† sau ƒë√≥ l·∫•y trung b√¨nh b·∫±ng c√°ch chia cho $m$ (t·ªïng s·ªë v√≠ d·ª•). ƒêi·ªÅu n√†y gi√∫p ƒë√°nh gi√° t·ªïng qu√°t h∆°n v·ªÅ hi·ªáu nƒÉng c·ªßa m√¥ h√¨nh".
>note: 
![[Pasted image 20241011103325.png]]
>trong ƒë√≥ $y^{(i)}$ l√† gi√° tr·ªã th·ª±c (true value/original label) v√† $a^{[2](i)}$ l√† gi√° tr·ªã ƒë·∫ßu ra c·ªßa b∆∞·ªõc forward propagation (c√°c ph·∫ßn t·ª≠ c·ªßa $A^{[2]}$).

ƒê·ªÉ t·ªëi gi·∫£m m·∫•t m√°t, m√¨nh d√πng gradient descent ƒë·ªÉ c·∫≠p nh·∫≠t c√°c tham s·ªë w, b c·ªßa c√°c l·ªõp tr'c ƒë√≥ (tr'c l·ªõp ƒë·∫ßu ra):
$$
\begin{align}
W^{[1]} &= W^{[1]} - \alpha \frac{\partial \mathcal{L} }{ \partial W^{[1]} },\\
b^{[1]} &= b^{[1]} - \alpha \frac{\partial \mathcal{L} }{ \partial b^{[1]} },\\
W^{[2]} &= W^{[2]} - \alpha \frac{\partial \mathcal{L} }{ \partial W^{[2]} },\\
b^{[2]} &= b^{[2]} - \alpha \frac{\partial \mathcal{L} }{ \partial b^{[2]} },\\
\tag{9}
\end{align}
$$
trong ƒë√≥ $\alpha$ l√† t·ªëc ƒë·ªô h·ªçc.

ƒê·ªÉ th·ª±c hi·ªán hu·∫•n luy·ªán, 