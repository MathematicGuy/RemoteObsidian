
"smart_sources:Transformer Model Introduction (Attention Is All You Need).md": {"path":"Transformer Model Introduction (Attention Is All You Need).md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"1kwfgxr","at":1768089397189},"class_name":"SmartSource","last_import":{"mtime":1766891456716,"size":13731,"at":1768089397190,"hash":"1kwfgxr"},"blocks":{"##Chapter 5: Transformer Models":[1,144],"##Chapter 5: Transformer Models#Word Embedding":[2,8],"##Chapter 5: Transformer Models#Word Embedding#{1}":[3,8],"##Chapter 5: Transformer Models#5.3 Positional Encoding":[9,36],"##Chapter 5: Transformer Models#5.3 Positional Encoding#{1}":[10,36],"##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism":[37,40],"##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism#{1}":[38,40],"##Chapter 5: Transformer Models#5.2 Self-Attention Encoder":[41,84],"##Chapter 5: Transformer Models#5.2 Self-Attention Encoder#{1}":[42,84],"##Chapter 5: Transformer Models#5.3 Transformers Decoder":[85,127],"##Chapter 5: Transformer Models#5.3 Transformers Decoder#{1}":[86,127],"##Chapter 5: Transformer Models#5.4 Lab: Implementing Attention, Self-Attention, Positional Encoding, and Transformer":[128,131],"##Chapter 5: Transformer Models#Side Note: LoRA":[132,144],"##Chapter 5: Transformer Models#Side Note: LoRA#{1}":[133,144]},"outlinks":[{"title":"Pasted image 20250314123831.png","target":"Pasted image 20250314123831.png","line":7,"embedded":true},{"title":"Pasted image 20250314125912.png","target":"Pasted image 20250314125912.png","line":21,"embedded":true},{"title":"Pasted image 20250314132711.png","target":"Pasted image 20250314132711.png","line":23,"embedded":true},{"title":"Pasted image 20250314130830.png","target":"Pasted image 20250314130830.png","line":25,"embedded":true},{"title":"Pasted image 20250314132957.png","target":"Pasted image 20250314132957.png","line":27,"embedded":true},{"title":"Pasted image 20250314133450.png","target":"Pasted image 20250314133450.png","line":28,"embedded":true},{"title":"Pasted image 20250314133753.png","target":"Pasted image 20250314133753.png","line":33,"embedded":true},{"title":"Pasted image 20250314134659.png","target":"Pasted image 20250314134659.png","line":35,"embedded":true},{"title":"Pasted image 20250314180207.png","target":"Pasted image 20250314180207.png","line":38,"embedded":true},{"title":"Pasted image 20250314180958.png","target":"Pasted image 20250314180958.png","line":44,"embedded":true},{"title":"Pasted image 20250314181759.png","target":"Pasted image 20250314181759.png","line":49,"embedded":true},{"title":"Pasted image 20250314182718.png","target":"Pasted image 20250314182718.png","line":50,"embedded":true},{"title":"Pasted image 20250314182924.png","target":"Pasted image 20250314182924.png","line":56,"embedded":true},{"title":"Pasted image 20250314184709.png","target":"Pasted image 20250314184709.png","line":59,"embedded":true},{"title":"Pasted image 20250314185911.png","target":"Pasted image 20250314185911.png","line":65,"embedded":true},{"title":"Residual Connections","target":"Residual Connections","line":74},{"title":"Pasted image 20250314192503.png","target":"Pasted image 20250314192503.png","line":76,"embedded":true},{"title":"Pasted image 20250314193136.png","target":"Pasted image 20250314193136.png","line":78,"embedded":true},{"title":" 333","target":"Pasted image 20250315091058.png# left","line":87,"embedded":true},{"title":" 203","target":"Pasted image 20250316072836.png","line":87,"embedded":true},{"title":"Pasted image 20250316072647.png","target":"Pasted image 20250316072647.png","line":99,"embedded":true},{"title":"Pasted image 20250316073303.png","target":"Pasted image 20250316073303.png","line":104,"embedded":true},{"title":"Pasted image 20250315094502.png","target":"Pasted image 20250315094502.png","line":106,"embedded":true},{"title":"Pasted image 20250315095301.png","target":"Pasted image 20250315095301.png","line":109,"embedded":true},{"title":"Pasted image 20250315095948.png","target":"Pasted image 20250315095948.png","line":112,"embedded":true},{"title":"Pasted image 20250316063403.png","target":"Pasted image 20250316063403.png","line":115,"embedded":true},{"title":"Pasted image 20250315184141.png","target":"Pasted image 20250315184141.png","line":120,"embedded":true},{"title":"Pasted image 20250316062619.png","target":"Pasted image 20250316062619.png","line":127,"embedded":true},{"title":"Pasted image 20251030204002.png","target":"Pasted image 20251030204002.png","line":133,"embedded":true},{"title":"Pasted image 20251030204952.png","target":"Pasted image 20251030204952.png","line":137,"embedded":true},{"title":" 433","target":"Pasted image 20251030205157.png","line":142,"embedded":true},{"title":" 433","target":"Pasted image 20251030205148.png","line":144,"embedded":true}],"task_lines":[],"tasks":{},"codeblock_ranges":[]},
"smart_sources:Transformer Model Introduction (Attention Is All You Need).md": {"path":"Transformer Model Introduction (Attention Is All You Need).md","embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.04534794,-0.03056616,-0.00414919,-0.04614604,-0.13675666,0.0641569,-0.03483279,0.00819034,0.01245316,0.03372176,-0.03770974,-0.054216,0.04837205,0.04880723,0.0342097,0.03802846,-0.06898551,0.06279328,-0.0974928,-0.06625788,0.16310656,-0.05879442,0.00564004,-0.02199414,0.01273496,0.05464142,-0.05693027,0.00526938,-0.01226267,-0.25438765,-0.03763537,-0.0644411,0.09698735,0.01975328,-0.06266002,0.00292431,-0.10460517,0.05228865,-0.01079319,0.02941749,0.02495107,0.03664561,0.02269001,-0.04355631,0.0461585,-0.06087328,-0.03111157,-0.00308892,-0.01045331,0.01220677,0.05940738,-0.03337679,-0.03567285,0.07812617,0.03392443,0.02753773,0.06978485,0.07187302,0.03282497,0.02546459,0.00623541,0.05655716,-0.17250118,0.11072556,0.01499053,-0.00002644,-0.04351784,-0.05669204,0.00615098,0.03769251,0.00969605,-0.023151,0.04267977,0.03820224,0.05493514,0.01666735,0.01093471,0.04927005,0.00414151,-0.00256953,0.0470485,0.02244503,-0.03118979,0.03236769,-0.0305792,-0.04332823,0.05811249,-0.05073935,-0.01379887,0.03981293,-0.02340757,-0.06252358,-0.04691632,0.04637359,-0.00389259,-0.0442421,0.02851358,0.03810841,-0.06323747,0.09210819,-0.03894897,0.01365894,-0.02359424,-0.04148046,0.0568889,-0.06927123,-0.02241414,-0.02096651,-0.04710731,0.00457392,-0.07933988,-0.00506015,-0.03584903,-0.06325199,-0.03719446,-0.02010844,0.03195664,0.01416085,0.0123407,-0.03330082,0.00570896,0.01036732,0.0065617,0.04413082,0.05175996,0.00909289,0.09984241,0.10528446,0.0810244,0.03926918,0.06027918,0.05976044,-0.05784985,0.06044938,-0.04451377,0.04879217,0.04446269,-0.00391046,-0.0177609,-0.00039925,0.02198507,0.01725958,-0.02797362,-0.06344875,-0.00749509,0.11144826,-0.07730316,-0.06232833,-0.01378275,-0.01616807,-0.01184522,0.03438047,-0.04449899,-0.0401586,-0.00160727,0.07473102,0.02774009,-0.03637929,-0.0623678,-0.01246233,0.00064188,-0.00812385,-0.03126738,0.03083938,0.02383376,-0.08649364,-0.02118821,-0.01049771,0.02993495,-0.10072657,0.04160063,-0.01108698,-0.04572676,0.01121886,0.0571035,0.04468358,-0.01262725,-0.02252794,0.00963478,0.04961772,0.01543633,-0.08324008,0.00224252,-0.00106254,-0.0021902,-0.03498849,0.00172147,-0.06666313,0.00593787,0.02568403,-0.00996609,0.00895233,-0.06166345,-0.03889253,-0.05269296,-0.03417009,-0.01238138,0.03148474,-0.01179611,-0.01387388,0.05230652,0.0186058,0.01593527,0.00755279,-0.02800359,-0.03192059,0.07960074,0.00123034,0.06735095,0.00573163,-0.05881941,0.01543674,0.05191392,-0.05136883,-0.0045086,0.00199514,0.01140163,0.04055906,0.01857404,-0.02361752,-0.03523426,-0.05969571,-0.03982862,-0.21316054,-0.04346096,0.03467065,-0.00003649,0.03919503,-0.08335703,0.06381901,-0.01868367,0.03641258,0.04982448,0.0906648,-0.01960631,-0.04113528,0.03539253,0.00305779,0.01828127,0.01493755,0.02746139,0.00830418,0.03564988,-0.00654264,0.00763497,0.0132247,-0.06585264,0.05723985,-0.0233058,0.16042875,0.01709778,0.07631808,0.06394062,0.05074422,-0.01564767,-0.02489802,-0.03729613,0.06716525,-0.01645439,0.00644561,0.04202541,0.02949856,-0.00597575,-0.02817118,0.01151166,-0.00873404,-0.09332047,-0.00415981,-0.00327347,-0.04949344,-0.02410232,-0.05652431,0.07901892,0.02353955,-0.01995105,0.05775604,0.01707396,-0.04124627,-0.06510147,-0.04556147,-0.00067299,-0.03122844,0.00410083,-0.00175214,-0.05117136,0.01510409,-0.04998799,-0.02112179,0.02150431,-0.03237364,-0.00701031,0.00546579,-0.01353623,-0.04251572,0.10373907,0.03030202,0.02387775,0.00527247,0.04917129,-0.03363851,0.00461073,-0.03737801,-0.00390457,0.03357533,-0.00117677,0.06322321,0.02865177,0.08421685,-0.00963725,0.05485931,0.00119281,0.0532692,0.06917726,-0.05375952,-0.00919916,0.00077815,0.04476966,-0.01393015,-0.05270744,-0.26006612,-0.01673155,0.060192,0.06635377,-0.03578551,-0.03083067,0.01649821,-0.0725032,-0.03145069,0.01036363,-0.0772093,0.0469344,0.07262843,0.00678926,-0.0370759,0.0125694,0.08182769,-0.05765848,0.01319357,-0.04805259,-0.00874431,0.01136616,0.20218033,-0.02170626,0.01674322,-0.02172924,-0.07417651,0.0128741,0.04963574,0.02737173,0.00694814,-0.03304792,0.07491214,-0.0041496,0.03621948,0.03300476,0.0043719,-0.01704121,0.07356946,0.02785283,0.03387458,0.03082348,-0.05210064,-0.02422425,0.05113405,0.00724078,0.03684172,0.00084288,-0.04645467,0.01910401,-0.00392017,0.04869857,0.00319006,-0.01780499,0.01773242,0.04387693,-0.0784175,-0.00930112,-0.02392031,-0.00662163,-0.0136716,-0.03868395,0.01707689,0.02081229,-0.00694241],"last_embed":{"hash":"1kwfgxr","tokens":502}}},"last_read":{"hash":"1kwfgxr","at":1768089443554},"class_name":"SmartSource","last_import":{"mtime":1766891456716,"size":13731,"at":1768089397190,"hash":"1kwfgxr"},"blocks":{"##Chapter 5: Transformer Models":[1,144],"##Chapter 5: Transformer Models#Word Embedding":[2,8],"##Chapter 5: Transformer Models#Word Embedding#{1}":[3,8],"##Chapter 5: Transformer Models#5.3 Positional Encoding":[9,36],"##Chapter 5: Transformer Models#5.3 Positional Encoding#{1}":[10,36],"##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism":[37,40],"##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism#{1}":[38,40],"##Chapter 5: Transformer Models#5.2 Self-Attention Encoder":[41,84],"##Chapter 5: Transformer Models#5.2 Self-Attention Encoder#{1}":[42,84],"##Chapter 5: Transformer Models#5.3 Transformers Decoder":[85,127],"##Chapter 5: Transformer Models#5.3 Transformers Decoder#{1}":[86,127],"##Chapter 5: Transformer Models#5.4 Lab: Implementing Attention, Self-Attention, Positional Encoding, and Transformer":[128,131],"##Chapter 5: Transformer Models#Side Note: LoRA":[132,144],"##Chapter 5: Transformer Models#Side Note: LoRA#{1}":[133,144]},"outlinks":[{"title":"Pasted image 20250314123831.png","target":"Pasted image 20250314123831.png","line":7,"embedded":true},{"title":"Pasted image 20250314125912.png","target":"Pasted image 20250314125912.png","line":21,"embedded":true},{"title":"Pasted image 20250314132711.png","target":"Pasted image 20250314132711.png","line":23,"embedded":true},{"title":"Pasted image 20250314130830.png","target":"Pasted image 20250314130830.png","line":25,"embedded":true},{"title":"Pasted image 20250314132957.png","target":"Pasted image 20250314132957.png","line":27,"embedded":true},{"title":"Pasted image 20250314133450.png","target":"Pasted image 20250314133450.png","line":28,"embedded":true},{"title":"Pasted image 20250314133753.png","target":"Pasted image 20250314133753.png","line":33,"embedded":true},{"title":"Pasted image 20250314134659.png","target":"Pasted image 20250314134659.png","line":35,"embedded":true},{"title":"Pasted image 20250314180207.png","target":"Pasted image 20250314180207.png","line":38,"embedded":true},{"title":"Pasted image 20250314180958.png","target":"Pasted image 20250314180958.png","line":44,"embedded":true},{"title":"Pasted image 20250314181759.png","target":"Pasted image 20250314181759.png","line":49,"embedded":true},{"title":"Pasted image 20250314182718.png","target":"Pasted image 20250314182718.png","line":50,"embedded":true},{"title":"Pasted image 20250314182924.png","target":"Pasted image 20250314182924.png","line":56,"embedded":true},{"title":"Pasted image 20250314184709.png","target":"Pasted image 20250314184709.png","line":59,"embedded":true},{"title":"Pasted image 20250314185911.png","target":"Pasted image 20250314185911.png","line":65,"embedded":true},{"title":"Residual Connections","target":"Residual Connections","line":74},{"title":"Pasted image 20250314192503.png","target":"Pasted image 20250314192503.png","line":76,"embedded":true},{"title":"Pasted image 20250314193136.png","target":"Pasted image 20250314193136.png","line":78,"embedded":true},{"title":" 333","target":"Pasted image 20250315091058.png# left","line":87,"embedded":true},{"title":" 203","target":"Pasted image 20250316072836.png","line":87,"embedded":true},{"title":"Pasted image 20250316072647.png","target":"Pasted image 20250316072647.png","line":99,"embedded":true},{"title":"Pasted image 20250316073303.png","target":"Pasted image 20250316073303.png","line":104,"embedded":true},{"title":"Pasted image 20250315094502.png","target":"Pasted image 20250315094502.png","line":106,"embedded":true},{"title":"Pasted image 20250315095301.png","target":"Pasted image 20250315095301.png","line":109,"embedded":true},{"title":"Pasted image 20250315095948.png","target":"Pasted image 20250315095948.png","line":112,"embedded":true},{"title":"Pasted image 20250316063403.png","target":"Pasted image 20250316063403.png","line":115,"embedded":true},{"title":"Pasted image 20250315184141.png","target":"Pasted image 20250315184141.png","line":120,"embedded":true},{"title":"Pasted image 20250316062619.png","target":"Pasted image 20250316062619.png","line":127,"embedded":true},{"title":"Pasted image 20251030204002.png","target":"Pasted image 20251030204002.png","line":133,"embedded":true},{"title":"Pasted image 20251030204952.png","target":"Pasted image 20251030204952.png","line":137,"embedded":true},{"title":" 433","target":"Pasted image 20251030205157.png","line":142,"embedded":true},{"title":" 433","target":"Pasted image 20251030205148.png","line":144,"embedded":true}],"task_lines":[],"tasks":{},"codeblock_ranges":[],"last_embed":{"hash":"1kwfgxr","at":1768089443433}},"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.03589865,-0.03040573,-0.00379952,-0.04561726,-0.14059423,0.06706356,-0.02819601,0.00357785,0.01444526,0.03195802,-0.04056881,-0.0529479,0.04914066,0.05770668,0.03123997,0.04069326,-0.06460296,0.05997804,-0.09203783,-0.06561663,0.15716858,-0.05609814,0.00654453,-0.01934011,0.01000764,0.04803155,-0.05445897,0.00945751,-0.0118003,-0.25334257,-0.04190646,-0.06890982,0.0996041,0.02306198,-0.05771805,0.00662802,-0.10306543,0.04958399,-0.01234128,0.03503491,0.02561409,0.03768324,0.01907669,-0.04346275,0.04222202,-0.06003144,-0.02820148,-0.00772408,-0.0151736,0.01601783,0.05767589,-0.02962947,-0.03607642,0.08133601,0.02625206,0.02261432,0.07614625,0.07598195,0.03777619,0.02650842,0.00646274,0.06021689,-0.17053291,0.10746666,0.01479339,-0.00360455,-0.04466373,-0.05164176,0.00895904,0.03976746,0.00738919,-0.02476774,0.0401029,0.0366129,0.05965606,0.02155865,0.00877373,0.05873422,0.00355146,-0.00039153,0.04781891,0.01183422,-0.032231,0.03416241,-0.03591237,-0.04681047,0.05603452,-0.05456162,-0.01849832,0.03681383,-0.02854428,-0.06309565,-0.04693259,0.0496376,0.00091818,-0.04428082,0.02937565,0.03350268,-0.07117505,0.09669423,-0.04534888,0.01840048,-0.0238445,-0.04160079,0.05123724,-0.06322383,-0.01964187,-0.01927532,-0.04424573,0.00231987,-0.07874583,-0.00836587,-0.0353327,-0.06718212,-0.03391452,-0.01412644,0.03417557,0.01460826,0.01115654,-0.02833329,-0.00204824,0.00614473,0.00737207,0.04289305,0.05156498,0.00953513,0.10460064,0.11054959,0.08461834,0.03959465,0.05679192,0.06880569,-0.05915022,0.05851253,-0.04711853,0.04808953,0.04537082,-0.00778791,-0.01531938,-0.00144773,0.01967538,0.02046233,-0.02637004,-0.068548,-0.00751593,0.10799526,-0.0756404,-0.06222693,-0.01466435,-0.01297398,-0.01876396,0.03715989,-0.04718293,-0.03915199,-0.00524712,0.07126129,0.0235932,-0.04083887,-0.05952593,-0.01194186,-0.00478116,-0.00098005,-0.02816223,0.03142262,0.02510657,-0.08141348,-0.02271298,-0.01430642,0.02842879,-0.10819732,0.04142548,-0.01388079,-0.04508925,0.01447995,0.05325799,0.04219852,-0.02161304,-0.02023616,0.00578258,0.04578854,0.01744694,-0.08216325,0.00635321,-0.00033446,-0.00617308,-0.03508871,-0.00142325,-0.0714374,0.00787425,0.0289461,-0.0073846,0.00988944,-0.06363235,-0.044722,-0.05608695,-0.03502201,-0.01445425,0.02661161,-0.01252711,-0.01053852,0.03459165,0.01712497,0.02222183,0.00655663,-0.03040989,-0.0284861,0.07155576,0.00409505,0.07332453,0.00373917,-0.05721014,0.01630519,0.04693776,-0.04641426,-0.00344804,-0.00078914,0.01158333,0.03959237,0.02286028,-0.03283783,-0.03485366,-0.06283206,-0.03612762,-0.2134309,-0.04531039,0.03864805,-0.00811612,0.04172442,-0.08716296,0.06764535,-0.01561213,0.02677407,0.04831048,0.08517934,-0.02050217,-0.03941345,0.03792965,0.00520524,0.01610521,0.01139654,0.02787578,0.00754342,0.03789112,-0.01128553,-0.00326151,0.00953672,-0.06529034,0.06379043,-0.02325477,0.15848701,0.01494884,0.07982524,0.07185413,0.05578261,-0.01573068,-0.02948925,-0.04140057,0.06720507,-0.01722514,0.00779438,0.03992926,0.02996376,-0.00446644,-0.02846375,0.01211747,-0.01186027,-0.09127478,-0.00788962,-0.00139086,-0.04382643,-0.02729766,-0.0629801,0.07985598,0.01889777,-0.02334602,0.05910702,0.02031766,-0.03938211,-0.064963,-0.04168336,0.00331091,-0.0312482,0.00608978,0.00120944,-0.05277145,0.01967454,-0.04775103,-0.02284106,0.01519287,-0.03118513,-0.00598103,0.00343242,-0.00649355,-0.04680483,0.10219961,0.02473528,0.02257201,0.00709241,0.05241379,-0.03430256,-0.00093683,-0.04016382,0.00012911,0.03553277,-0.00340523,0.06453905,0.02911353,0.08939534,-0.00758033,0.05962951,0.00195002,0.05370174,0.07459554,-0.05202594,-0.01118397,0.00042088,0.04913088,-0.0084577,-0.0548404,-0.25455168,-0.01496459,0.06241407,0.07130077,-0.02896645,-0.0253692,0.01082045,-0.06991093,-0.03305053,0.01061728,-0.06930744,0.04245346,0.06972545,0.01291149,-0.03435388,0.01412374,0.07666858,-0.05456864,0.01864297,-0.04812329,-0.00591873,0.01306504,0.19811566,-0.02341342,0.01384208,-0.01851922,-0.07501876,0.01244006,0.04209677,0.02309689,0.00859644,-0.02817672,0.08050229,-0.00322932,0.03924496,0.03776133,0.00767246,-0.01262638,0.07558712,0.02740706,0.03808871,0.02127296,-0.05310006,-0.02217183,0.04661273,0.00852178,0.04137883,-0.00346339,-0.05249228,0.01962844,-0.00467526,0.05276416,0.0088432,-0.02260248,0.0181845,0.03911234,-0.07979207,-0.00918551,-0.02702852,-0.00194623,-0.01393811,-0.03252375,0.01311471,0.0202684,-0.00562211],"last_embed":{"hash":"1kwfgxr","tokens":415}}},"text":null,"length":0,"last_read":{"hash":"1kwfgxr","at":1768089443456},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models","lines":[1,144],"size":13713,"outlinks":[{"title":"Pasted image 20250314123831.png","target":"Pasted image 20250314123831.png","line":7,"embedded":true},{"title":"Pasted image 20250314125912.png","target":"Pasted image 20250314125912.png","line":21,"embedded":true},{"title":"Pasted image 20250314132711.png","target":"Pasted image 20250314132711.png","line":23,"embedded":true},{"title":"Pasted image 20250314130830.png","target":"Pasted image 20250314130830.png","line":25,"embedded":true},{"title":"Pasted image 20250314132957.png","target":"Pasted image 20250314132957.png","line":27,"embedded":true},{"title":"Pasted image 20250314133450.png","target":"Pasted image 20250314133450.png","line":28,"embedded":true},{"title":"Pasted image 20250314133753.png","target":"Pasted image 20250314133753.png","line":33,"embedded":true},{"title":"Pasted image 20250314134659.png","target":"Pasted image 20250314134659.png","line":35,"embedded":true},{"title":"Pasted image 20250314180207.png","target":"Pasted image 20250314180207.png","line":38,"embedded":true},{"title":"Pasted image 20250314180958.png","target":"Pasted image 20250314180958.png","line":44,"embedded":true},{"title":"Pasted image 20250314181759.png","target":"Pasted image 20250314181759.png","line":49,"embedded":true},{"title":"Pasted image 20250314182718.png","target":"Pasted image 20250314182718.png","line":50,"embedded":true},{"title":"Pasted image 20250314182924.png","target":"Pasted image 20250314182924.png","line":56,"embedded":true},{"title":"Pasted image 20250314184709.png","target":"Pasted image 20250314184709.png","line":59,"embedded":true},{"title":"Pasted image 20250314185911.png","target":"Pasted image 20250314185911.png","line":65,"embedded":true},{"title":"Residual Connections","target":"Residual Connections","line":74},{"title":"Pasted image 20250314192503.png","target":"Pasted image 20250314192503.png","line":76,"embedded":true},{"title":"Pasted image 20250314193136.png","target":"Pasted image 20250314193136.png","line":78,"embedded":true},{"title":" 333","target":"Pasted image 20250315091058.png# left","line":87,"embedded":true},{"title":" 203","target":"Pasted image 20250316072836.png","line":87,"embedded":true},{"title":"Pasted image 20250316072647.png","target":"Pasted image 20250316072647.png","line":99,"embedded":true},{"title":"Pasted image 20250316073303.png","target":"Pasted image 20250316073303.png","line":104,"embedded":true},{"title":"Pasted image 20250315094502.png","target":"Pasted image 20250315094502.png","line":106,"embedded":true},{"title":"Pasted image 20250315095301.png","target":"Pasted image 20250315095301.png","line":109,"embedded":true},{"title":"Pasted image 20250315095948.png","target":"Pasted image 20250315095948.png","line":112,"embedded":true},{"title":"Pasted image 20250316063403.png","target":"Pasted image 20250316063403.png","line":115,"embedded":true},{"title":"Pasted image 20250315184141.png","target":"Pasted image 20250315184141.png","line":120,"embedded":true},{"title":"Pasted image 20250316062619.png","target":"Pasted image 20250316062619.png","line":127,"embedded":true},{"title":"Pasted image 20251030204002.png","target":"Pasted image 20251030204002.png","line":133,"embedded":true},{"title":"Pasted image 20251030204952.png","target":"Pasted image 20251030204952.png","line":137,"embedded":true},{"title":" 433","target":"Pasted image 20251030205157.png","line":142,"embedded":true},{"title":" 433","target":"Pasted image 20251030205148.png","line":144,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"1kwfgxr","at":1768089443456}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#Word Embedding#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.03106429,-0.01452643,-0.00813985,-0.03733686,-0.13240807,0.05293101,-0.00759958,0.00850128,0.00937396,-0.00118923,-0.03160474,-0.05059653,0.03615663,0.06627043,0.02163185,0.05220325,-0.0354002,0.04110346,-0.08601963,-0.06978197,0.12343919,-0.04623484,-0.00885684,-0.00327239,0.00904436,0.01068864,-0.0512982,0.02893368,-0.04255991,-0.25772595,-0.01129754,-0.09259,0.09209377,0.02985196,-0.04376719,0.02456746,-0.08633557,0.02670497,-0.02907937,0.03375132,0.02613927,0.01933043,0.03525856,-0.01485622,0.03763233,-0.05895393,-0.01217428,-0.02013729,-0.00843278,0.03138313,0.05012764,-0.02360716,-0.03305783,0.07020582,0.01975729,0.01153761,0.092049,0.08983729,0.05753933,0.02647508,0.00131491,0.07310018,-0.17667863,0.09655301,0.02356494,-0.00109091,-0.0643293,-0.06789197,0.01276348,0.04756084,0.01299806,-0.01615416,0.02251467,0.04318265,0.05752695,0.0499495,-0.01972211,0.05551955,0.01885367,0.00327608,0.05889397,-0.01797307,-0.0451525,0.0154724,-0.03789983,-0.0385619,0.04003582,-0.0486828,-0.01327302,0.01796768,-0.01847761,-0.05683399,-0.02233951,0.05299088,-0.02371357,-0.03826592,0.03278613,0.01821813,-0.07738355,0.11960174,-0.05423294,-0.00208053,-0.02507821,-0.04371226,0.03821128,-0.05718965,-0.01395224,-0.01823835,-0.05352739,-0.00985993,-0.05331571,-0.01876117,-0.00483734,-0.05321458,-0.02964702,-0.02210093,0.05036054,0.01577396,-0.0000155,-0.0315896,-0.00015019,-0.01619761,0.03404775,0.03066144,0.02862498,0.01626327,0.09441582,0.11665276,0.09451312,0.04090154,0.04845915,0.0665933,-0.07764442,0.04091034,-0.03392716,0.03704345,0.05822813,-0.01658403,-0.0065391,0.00123967,0.01684647,0.00893627,-0.02614937,-0.07215711,-0.02031347,0.10643335,-0.02850682,-0.06683587,-0.02401128,-0.00936763,-0.03057197,0.03435625,-0.03312226,-0.02964416,0.01119616,0.06087542,0.01891526,-0.06711779,-0.06558216,-0.00426621,0.00106335,0.01130723,-0.02235514,0.04035853,0.02645151,-0.0758164,-0.03043683,-0.02468829,0.02464966,-0.13084958,0.02831575,-0.04082709,-0.06890976,0.01610647,0.05802481,0.03293338,-0.03032751,-0.02086663,-0.00638606,0.05136493,0.00836087,-0.08960603,0.00807645,-0.00089661,0.00763455,-0.03269134,-0.0002528,-0.09388174,0.02012283,0.04353758,-0.0423455,0.03908985,-0.0564626,-0.06176176,-0.05585995,-0.0521251,-0.04173628,0.0135312,-0.01184258,-0.01051388,0.00087033,0.03118823,0.05695243,0.00006619,-0.03002273,-0.00190404,0.01891042,0.02538485,0.06941742,-0.00175081,-0.04912968,0.03452909,0.05418956,-0.03769733,-0.00173404,-0.01489876,0.02671151,0.04398137,0.00726393,-0.05014108,-0.01308733,-0.06108988,-0.01301616,-0.21578914,-0.032462,0.05334523,-0.0301495,0.05887131,-0.08202147,0.07834517,-0.03683183,0.00542248,0.06147105,0.07443549,-0.00095341,-0.03614359,0.02714905,0.01747064,0.02403687,0.00167081,0.0234664,0.01695469,0.03629745,-0.01295406,-0.01210999,0.04819404,-0.0614084,0.06862789,0.00822606,0.1444626,0.00011558,0.10220889,0.07188564,0.05269061,-0.01934396,-0.02933071,-0.05736271,0.07233048,-0.0477019,0.02567539,0.02444093,0.03388288,-0.02454608,-0.02661589,0.00890039,-0.01495002,-0.09497414,-0.012027,0.00437132,-0.02924377,-0.0164134,-0.06552929,0.08794726,0.01148819,-0.00831724,0.05295066,0.03777119,-0.02612284,-0.06491648,-0.04250765,0.02195829,-0.03373558,0.019838,-0.01059607,-0.03478959,0.00297911,-0.05900047,-0.03220122,0.00951665,-0.02938402,-0.01264885,0.01363918,-0.0055447,-0.0552841,0.10794356,0.03873357,0.02871674,0.02497992,0.05819717,-0.04892288,-0.03220423,-0.05194929,0.00225344,0.03690218,0.01779331,0.05272204,0.03951989,0.09452571,0.01130395,0.03555696,-0.0091762,0.02466058,0.06810221,-0.04737189,-0.01563556,0.01072871,0.06005374,0.0061137,-0.05209531,-0.25594276,-0.02431783,0.04713223,0.06426971,-0.01835561,-0.00213648,0.02366654,-0.05045532,-0.0350205,-0.01453857,-0.04750872,0.02757672,0.05928207,0.03733882,-0.04540269,0.00258362,0.07587502,-0.04969694,0.02435724,-0.06490522,0.01642787,0.01878014,0.18358651,-0.02487485,0.00171468,0.00448892,-0.06436454,-0.00154212,0.04863093,0.02652743,0.01061396,-0.01873616,0.06037948,-0.01702533,0.05084394,0.05857111,0.01331304,0.00642491,0.06912525,0.02500975,0.04558073,0.01905647,-0.03519423,-0.02682924,0.03045376,0.03751383,0.03442788,-0.01674416,-0.09673937,0.03406589,0.01027787,0.03513607,0.02110498,-0.0193948,0.03186638,0.05658523,-0.07084661,-0.00077267,-0.04204731,0.00720124,-0.00515891,0.00681077,0.00369379,0.01478532,-0.00923978],"last_embed":{"hash":"1i3ic61","tokens":177}}},"text":null,"length":0,"last_read":{"hash":"1i3ic61","at":1768089443479},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#Word Embedding#{1}","lines":[3,8],"size":569,"outlinks":[{"title":"Pasted image 20250314123831.png","target":"Pasted image 20250314123831.png","line":5,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"1i3ic61","at":1768089443479}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.3 Positional Encoding#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.0568388,-0.04225756,0.01328304,-0.03491337,-0.10041502,0.05562141,-0.06163073,-0.00332018,0.0186947,0.03624201,-0.03132772,-0.03966029,0.03900428,0.03146445,0.02804365,0.01049611,-0.07961471,0.06707964,-0.08906513,-0.06934847,0.19083598,-0.05810947,0.01869328,-0.04964957,0.02088236,0.08578927,-0.04969283,-0.01310854,0.02013674,-0.24542549,-0.06843067,-0.04058714,0.06764523,0.0056218,-0.05422251,-0.03861995,-0.10446426,0.09653062,-0.01690189,0.04928705,0.0056944,0.02794423,0.00322098,-0.05969273,0.02755433,-0.04506928,-0.02255901,0.00785222,-0.01427754,-0.04970856,0.05780093,-0.04014914,-0.05034171,0.08457305,0.04223209,0.04795304,0.06340991,0.05271768,0.00223753,0.02060496,0.01938185,0.04791553,-0.17106865,0.10549153,0.00912208,0.02380805,-0.00846362,-0.02533916,-0.00642285,0.03421851,-0.01982383,-0.03669851,0.02854463,0.01722029,0.05947094,-0.0113959,0.02338172,0.0250013,-0.02728261,-0.01251916,0.02267244,0.05611979,-0.00614537,0.01941751,-0.03603067,-0.06292772,0.06165137,-0.04330509,-0.02359441,0.05719212,-0.02991124,-0.0523037,-0.07097398,0.03689163,-0.0021486,-0.04270159,0.03611299,0.06049711,-0.05027304,0.0873528,-0.01826969,0.0265794,-0.01892755,-0.03530065,0.05864374,-0.0527191,-0.04022096,0.0041591,-0.04712947,0.01384044,-0.0792872,0.01393772,-0.06429649,-0.07353407,-0.04015011,-0.02868793,0.04228409,0.00542916,0.02277117,-0.0364003,-0.00131767,0.03568993,0.0035655,0.06666781,0.05718752,-0.0066086,0.10243065,0.06940864,0.07162537,0.04730356,0.06053719,0.04973404,-0.0316871,0.03822052,-0.04366612,0.05185659,0.03474049,0.01036567,-0.01351462,0.00013187,0.01792091,-0.00544878,-0.02144109,-0.02934711,-0.00155187,0.10277369,-0.10877752,-0.04217424,0.00384116,0.01573451,-0.01385859,0.04631032,-0.04894998,-0.04138343,-0.0146084,0.06799971,0.02872707,-0.02481403,-0.05925611,-0.02756502,0.01815069,-0.03112823,-0.05521711,0.0594012,0.01157532,-0.07551496,-0.0115225,0.02145503,0.02698287,-0.06074924,0.05210369,0.03147287,-0.02333669,0.02090626,0.04370997,0.06080756,-0.01505499,-0.01841998,0.00572268,0.04708114,0.03032477,-0.05687919,-0.00934409,0.02297863,-0.01793667,-0.03033785,-0.00968632,-0.03319658,-0.00643246,0.01110994,0.01997514,-0.00688354,-0.0545007,-0.01223997,-0.04443931,-0.01066782,0.00505049,0.02571797,-0.00815865,-0.03429299,0.07014891,-0.00024213,-0.01276617,0.02518422,-0.02539957,-0.05007966,0.10537566,-0.00907226,0.05925123,0.00882097,-0.06458003,-0.00433549,0.03881928,-0.03412613,-0.00598413,0.03338031,0.01772115,0.02571699,0.03642818,0.01335576,-0.05930252,-0.05037123,-0.03793135,-0.21395642,-0.03179446,-0.00295628,0.00520125,0.02584316,-0.07210357,0.01068523,0.00997165,0.05468271,0.06354385,0.0915015,-0.05313479,-0.03273103,0.05341995,-0.00955049,0.0155034,0.01936298,0.03913451,-0.01500286,0.01809857,0.02634623,0.01606365,-0.01238479,-0.07579254,0.05200492,-0.05209907,0.17114948,0.01670446,0.05660709,0.02962001,0.03919265,0.00685857,-0.02625282,-0.01689661,0.03640939,0.01009673,-0.02586049,0.03525715,0.02178544,-0.00462504,-0.03702047,0.01357019,0.00912152,-0.06643068,-0.00604863,-0.01952809,-0.06373708,-0.01480596,-0.01991236,0.08819776,0.02747521,-0.0399153,0.05987655,-0.01084105,-0.04333747,-0.04372305,-0.03673142,-0.04008821,-0.03302592,-0.00136005,0.00747782,-0.05321826,0.01076833,-0.04263004,-0.02893885,0.02620393,-0.02815699,0.00523092,0.00700387,-0.00810134,-0.01076655,0.10139041,0.01992662,0.00029897,0.00554279,0.05243922,-0.00796997,0.00554165,-0.00564739,-0.01695941,0.03991491,-0.02291008,0.06559582,0.02100842,0.09162883,-0.02121082,0.05823096,0.0246503,0.06912658,0.04779439,-0.03162925,-0.00595108,-0.00963936,0.01065633,-0.03091616,-0.0820792,-0.24938646,0.02436953,0.05570043,0.07994536,-0.03421756,-0.03854288,-0.00479012,-0.08559493,-0.02255969,0.03362897,-0.06818879,0.06479092,0.06943206,-0.03715803,-0.03090952,0.04527085,0.08617332,-0.08342282,0.00413713,-0.04008583,-0.01136435,0.00096807,0.21264453,-0.00707553,0.01928796,-0.04649234,-0.05673996,0.03063312,0.02702681,0.02399823,0.01000067,-0.04419341,0.09763715,0.0178716,0.02009286,0.01387019,0.01588183,-0.02479081,0.07874221,0.03711386,0.0145949,0.02506255,-0.06513885,-0.02918714,0.08203618,-0.00227199,0.02054034,0.00423471,-0.00823157,0.01057598,-0.02608147,0.03365573,-0.00924926,-0.01941069,0.0198671,0.0272894,-0.07811616,-0.00350058,-0.01242942,-0.02308381,-0.01703422,-0.08572183,0.0179239,0.0152283,-0.00415127],"last_embed":{"hash":"1ktptvk","tokens":447}}},"text":null,"length":0,"last_read":{"hash":"1ktptvk","at":1768089443489},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.3 Positional Encoding#{1}","lines":[10,36],"size":2823,"outlinks":[{"title":"Pasted image 20250314125912.png","target":"Pasted image 20250314125912.png","line":12,"embedded":true},{"title":"Pasted image 20250314132711.png","target":"Pasted image 20250314132711.png","line":14,"embedded":true},{"title":"Pasted image 20250314130830.png","target":"Pasted image 20250314130830.png","line":16,"embedded":true},{"title":"Pasted image 20250314132957.png","target":"Pasted image 20250314132957.png","line":18,"embedded":true},{"title":"Pasted image 20250314133450.png","target":"Pasted image 20250314133450.png","line":19,"embedded":true},{"title":"Pasted image 20250314133753.png","target":"Pasted image 20250314133753.png","line":24,"embedded":true},{"title":"Pasted image 20250314134659.png","target":"Pasted image 20250314134659.png","line":26,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"1ktptvk","at":1768089443489}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05970389,-0.02873587,-0.00523632,-0.01386933,-0.08846458,-0.00841002,0.03195676,0.02606096,0.03336341,-0.02096763,0.00434457,-0.00633827,0.03831741,0.08808087,0.01677583,0.02616058,-0.02121883,0.04237495,-0.0648083,-0.09938895,0.15454176,-0.05105959,-0.02607268,0.01509637,0.04696964,0.05007559,-0.01389092,0.03536674,-0.0150769,-0.21915561,-0.02897632,-0.04611457,0.08182476,0.01634091,-0.07218184,-0.01045539,-0.04940516,0.04629491,-0.01764466,0.02566336,0.03617665,0.0116448,0.00944086,0.02362758,-0.00629624,-0.03671827,-0.01391319,-0.0343235,-0.03399701,-0.06288192,0.01235568,-0.08920996,-0.04957618,0.03669801,0.03509412,0.05337097,0.1283918,0.03561614,0.06997183,-0.03018357,0.07109541,0.05137569,-0.17436628,0.1131119,0.02020678,0.01742799,-0.05669459,-0.05062813,0.03724049,0.05007978,-0.01410995,-0.01225171,-0.01683032,0.00380936,0.03060215,-0.00336985,0.0132658,0.00631109,0.00069686,0.0226788,0.00977925,0.04771841,-0.02680641,-0.01725514,-0.04179057,0.00064606,0.04157703,-0.02185998,0.01988536,-0.02435351,-0.06076142,-0.07211009,-0.02012438,0.01309313,-0.01113726,-0.04736312,0.01803089,0.00182878,-0.10023513,0.12328526,-0.03812302,-0.01309885,0.02201402,0.01184908,0.05010541,-0.02939705,-0.0225162,-0.01759535,-0.02794936,0.02750612,0.01160444,-0.00801514,-0.06861182,-0.08266589,0.04441716,-0.05096799,0.10172789,0.03908373,0.01329908,-0.08119959,0.00929636,0.0259274,0.0280647,0.04020806,0.01877907,-0.00299486,0.07066196,0.0530364,0.01794248,0.03213076,0.05392104,0.04680859,-0.07991057,0.01251542,-0.0186967,0.01625032,0.05283492,-0.01645665,0.01787858,-0.03628425,0.04302905,-0.04325,0.0363946,-0.06907906,-0.0241471,0.14259169,-0.04480181,-0.00706558,0.01330767,0.0019405,-0.03686957,0.05125936,-0.01040796,-0.0427248,-0.02874025,0.02762566,0.02556662,-0.0483154,-0.05126755,-0.00872558,-0.04495459,0.01769444,-0.04368639,0.07387494,-0.00016356,-0.07256532,-0.04206281,-0.00543389,0.00151079,-0.1045494,0.06690804,0.00953185,-0.04068611,0.02306545,0.03794789,0.03321981,-0.01953754,0.02496325,-0.02122033,0.07478119,0.04682638,-0.07742857,-0.00172601,0.02240315,0.00020019,-0.04162777,-0.00962102,-0.06972851,0.00680779,0.08289736,-0.0090172,0.04537953,0.00820867,-0.02368224,-0.06842317,0.01469535,-0.04232207,-0.01281267,-0.04541489,-0.02498731,-0.04268683,0.01963431,-0.01202758,0.01665661,-0.01519451,0.00745361,0.02825947,-0.01044626,0.09479751,0.00507308,-0.07859682,-0.03178712,0.01564738,-0.00971444,-0.01345129,0.00154196,0.016059,0.07336041,-0.02059464,0.04294987,0.02245222,-0.02424528,-0.03784591,-0.20171447,-0.05030145,0.02387713,0.03966874,0.01642301,-0.07827834,0.03940617,-0.01835761,0.01456432,0.07940625,0.0855543,-0.07388669,-0.04337623,0.03866795,-0.03353966,-0.01257921,-0.01005688,0.05791511,-0.02039719,0.02338956,0.04013587,0.00366486,0.04226261,-0.11090545,0.02261911,-0.04161016,0.1427048,0.01122761,0.05713156,0.05833567,-0.01085781,-0.03354375,-0.04949759,-0.05049969,-0.00883231,-0.04507742,0.00871184,-0.05853462,0.00245216,-0.02401466,-0.08302122,-0.01097022,0.00544256,-0.03171942,-0.0266875,-0.00844612,-0.04204687,0.00423992,-0.01903105,0.08244074,0.07503948,-0.04658965,0.06126549,-0.00301815,-0.01633358,-0.06297762,-0.04466398,-0.04299507,-0.04596489,0.00829924,-0.06855823,0.00047491,0.0190207,-0.05215623,0.04410693,0.00559159,-0.01007358,0.03398471,-0.02311316,0.02090943,-0.00781976,0.10760131,0.02318987,-0.00487926,0.00823219,0.04701767,0.01144901,-0.04594706,-0.04752294,-0.00441573,0.05283906,-0.02570564,0.0510858,0.0396282,0.07530708,0.04518789,0.05809585,0.02247422,0.0700004,0.02138117,-0.03720306,-0.00956431,0.00697066,-0.00785015,0.00629249,-0.02370076,-0.25822777,0.04140258,0.00962464,0.11369523,-0.02394774,-0.012168,0.03200262,-0.01530821,0.05655704,-0.01736082,-0.05042049,0.04216275,0.05110998,-0.047431,-0.08456472,0.0474174,0.08428205,-0.06875686,0.01786298,-0.03505761,0.02309118,-0.00950605,0.20791967,-0.01340349,-0.02540027,-0.05530819,-0.0488165,-0.01286665,0.00524256,0.02697029,0.01049176,0.00032961,0.03997845,0.01574506,0.04710672,0.02358673,0.04332832,0.02213776,0.05266068,0.02494111,0.01599599,0.03616541,-0.05855016,-0.03955312,0.14161247,-0.01152463,-0.02532453,-0.04796061,-0.06556769,0.03611242,0.01317663,-0.01030896,0.00153794,0.01463504,0.04787362,0.0384615,-0.07226994,0.035549,-0.02040037,0.00768333,0.00509276,-0.00945687,0.03533828,0.00404259,0.05092361],"last_embed":{"hash":"7e06b0","tokens":176}}},"text":null,"length":0,"last_read":{"hash":"7e06b0","at":1768089443506},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.1 Self-Attention Mechanism#{1}","lines":[38,40],"size":596,"outlinks":[{"title":"Pasted image 20250314180207.png","target":"Pasted image 20250314180207.png","line":1,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"7e06b0","at":1768089443506}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.2 Self-Attention Encoder#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.06596667,-0.06039811,-0.0400149,-0.06124659,-0.10425231,0.01589382,-0.02817079,0.02977821,0.01999048,0.00036852,0.00778781,-0.05545689,0.03292645,0.05645616,0.03114158,0.01019983,-0.05039146,0.01542475,-0.11508344,-0.05698455,0.15070812,-0.03827206,0.00225235,-0.02618991,-0.00366622,0.02762466,-0.05259312,0.00873713,-0.01986597,-0.24615815,-0.02337054,-0.0241816,0.03172749,0.00977578,-0.04911082,-0.02912499,-0.0513102,0.03334961,-0.02385231,0.06373516,-0.00025384,-0.0293151,0.01262587,0.01550826,0.01963508,-0.05458177,-0.0137025,-0.03658324,-0.03368348,-0.02280135,-0.00054953,-0.00275738,-0.05278519,0.02316879,0.04996515,0.03318486,0.10966364,0.06976567,0.03022059,-0.00037448,0.07570971,0.06021808,-0.1608465,0.10690577,0.01177928,0.05528117,-0.0434169,-0.05942827,-0.01251961,0.01811895,0.00028632,-0.01708429,0.0034347,0.03082704,0.04177042,0.0121258,-0.02533988,0.04071342,0.02656819,-0.00546706,0.0193971,0.01469188,-0.01054912,0.00354851,-0.01474508,-0.03816496,0.00023738,-0.0286014,0.03179564,0.02432112,-0.0065452,-0.06738881,-0.04089804,0.05855466,-0.01529425,-0.00636861,0.00342444,0.05355067,-0.07976499,0.0940451,-0.00837791,-0.00286324,0.0055959,-0.07083084,0.0648192,-0.09761664,-0.03999882,-0.00603104,-0.03092354,-0.01758042,-0.01141172,0.01970129,-0.02903359,-0.05404184,-0.0428969,-0.02310811,0.04951346,0.00922283,0.01457042,-0.0849691,0.00107762,-0.02238492,0.03264419,0.03052169,0.03520148,0.00688036,0.07484432,0.06197291,0.03225455,0.03310606,0.04205856,0.06631879,-0.07771647,0.01953458,-0.02302334,-0.00135751,0.05526837,0.03790537,0.00812917,-0.01205764,0.04261968,-0.04044092,-0.02675879,-0.06135384,-0.03094252,0.08367318,-0.07030998,-0.06329414,-0.02372622,-0.03024929,-0.01193489,0.04705886,-0.01518352,-0.08109289,-0.0023643,0.0554166,0.06799884,-0.03422866,-0.08457709,-0.06760053,-0.01017943,-0.01550759,-0.05335182,0.04339498,0.01530846,-0.05041144,-0.00018069,-0.01258469,0.01432578,-0.06767139,0.04053379,-0.04463458,-0.03828166,0.01952991,0.08437413,0.03580942,-0.05135144,0.00309279,-0.01938353,0.08384544,-0.00260467,-0.05464898,-0.015802,-0.00808494,-0.00443792,-0.03873395,0.01849677,-0.04365953,-0.03837585,0.07814156,-0.00165788,0.01543707,-0.02073995,-0.00091397,-0.03795055,-0.01769072,-0.03026263,0.01997588,-0.01767695,-0.0179782,0.04929519,0.02826308,0.0083203,-0.0036549,-0.01609882,-0.02047772,0.04695901,0.00396344,0.0455179,0.01293576,-0.03079017,0.01284196,0.04840798,-0.02883608,-0.00340041,0.02442692,0.02096573,0.05526612,-0.0011246,0.02237686,-0.01365855,-0.05391277,-0.04697669,-0.21395916,0.00100747,0.02040565,-0.01153814,0.039353,-0.06904144,0.02727988,-0.00051294,-0.00007797,0.08732104,0.07145127,-0.04326757,-0.04977142,0.07678784,-0.03320866,-0.0078826,-0.02592708,0.08428052,-0.00421956,0.0518091,0.01167339,0.01405183,0.07407861,-0.10037085,0.05097449,-0.03172155,0.19258244,-0.00209343,0.08987166,0.06261658,0.00965281,0.00650081,-0.0245766,-0.01987865,0.0108867,-0.04039079,0.01843283,0.03114162,0.02356496,0.00520814,-0.02956795,-0.02345293,0.00626886,-0.07026712,0.00133948,-0.00804313,-0.04092853,-0.00483529,-0.01859961,0.1085699,0.02253903,0.00386329,0.07431201,-0.00499907,0.00973907,-0.08260474,-0.07279907,-0.04060403,-0.02645991,0.0310494,-0.03423597,-0.05000737,0.00456979,-0.06513271,-0.00524243,0.00762005,-0.00306844,0.00784749,-0.00606348,-0.01725603,-0.02520969,0.07413186,0.00436744,0.03347846,0.0428742,0.10961814,-0.00263743,-0.00443346,-0.02463457,-0.01669678,0.01971636,-0.01822769,0.05267864,-0.00688924,0.09220942,0.00579508,0.06025146,0.01820663,0.03279189,0.01100336,-0.04914065,-0.04459256,0.02085145,-0.00202812,-0.01299405,-0.10698896,-0.26516837,0.00437273,0.03345413,0.05748625,-0.01207028,-0.00122812,0.04756734,-0.05127222,0.00490092,-0.03557882,-0.04691063,0.0329655,0.06665952,-0.00158974,-0.03764447,0.02639699,0.0801523,-0.06614424,0.03914094,-0.05068003,0.0279534,0.01204865,0.21051353,-0.01372446,0.01999517,-0.01140502,-0.04197042,0.04573734,0.04533233,0.05050258,0.02153856,-0.02489195,0.10543423,-0.04324451,0.02644126,0.05649614,-0.03666559,-0.00859478,0.08273185,0.04062666,0.05143052,0.02786418,-0.03819394,-0.03718351,0.13827395,0.02153964,0.01164831,-0.03214285,-0.04209264,0.05009436,0.0002686,0.04024989,0.00649543,-0.01680532,0.04661361,0.05788372,-0.09361742,0.02648929,-0.02344539,-0.010158,-0.03064269,-0.02872407,0.04791513,0.00654012,0.02049604],"last_embed":{"hash":"6vxjnu","tokens":468}}},"text":null,"length":0,"last_read":{"hash":"6vxjnu","at":1768089443521},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.2 Self-Attention Encoder#{1}","lines":[42,84],"size":4640,"outlinks":[{"title":"Pasted image 20250314180958.png","target":"Pasted image 20250314180958.png","line":3,"embedded":true},{"title":"Pasted image 20250314181759.png","target":"Pasted image 20250314181759.png","line":8,"embedded":true},{"title":"Pasted image 20250314182718.png","target":"Pasted image 20250314182718.png","line":9,"embedded":true},{"title":"Pasted image 20250314182924.png","target":"Pasted image 20250314182924.png","line":15,"embedded":true},{"title":"Pasted image 20250314184709.png","target":"Pasted image 20250314184709.png","line":18,"embedded":true},{"title":"Pasted image 20250314185911.png","target":"Pasted image 20250314185911.png","line":24,"embedded":true},{"title":"Residual Connections","target":"Residual Connections","line":33},{"title":"Pasted image 20250314192503.png","target":"Pasted image 20250314192503.png","line":35,"embedded":true},{"title":"Pasted image 20250314193136.png","target":"Pasted image 20250314193136.png","line":37,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"6vxjnu","at":1768089443521}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.3 Transformers Decoder#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.05729332,-0.0455667,0.02803616,-0.05794951,-0.0814416,-0.03977493,-0.01478379,0.02674426,0.00565686,0.02627489,0.01564737,-0.05604544,-0.01836254,0.05283049,-0.01278449,0.03748403,-0.05676575,0.03632227,-0.05371286,-0.06235588,0.13573447,-0.02519473,-0.00033941,-0.00708454,-0.00777666,0.03515116,-0.02083711,0.00569347,-0.03153052,-0.23002169,-0.03107387,-0.03211014,0.05229433,0.0279215,-0.04434723,-0.02212358,-0.07921045,0.01873595,-0.04529331,0.02635419,0.01506887,-0.00269993,-0.00150298,-0.01112785,-0.00763748,-0.07447646,-0.01831212,-0.01586262,-0.03211079,-0.03164887,0.00520317,-0.05534111,-0.04553305,0.01794416,0.03565941,0.01830801,0.12919988,0.04257375,0.06749856,0.01233127,0.03234395,0.09375324,-0.18618695,0.11920834,-0.00563484,0.03007715,-0.0018784,-0.02422647,-0.0127396,0.04868082,-0.0252292,-0.02922315,-0.00489151,0.03209537,0.03779802,0.04268108,-0.00593116,0.0157833,0.04087256,0.00599271,0.05850807,0.04203264,-0.01918841,0.02630194,-0.05668603,0.00137721,0.02307384,0.00759994,0.00414302,0.01520534,-0.05684241,-0.05152015,-0.03276687,0.06956007,-0.03669419,-0.03929889,0.02375368,0.06573941,-0.08433297,0.11627331,-0.0013677,-0.0222331,0.00520332,-0.06599881,0.05148936,-0.05484588,-0.00953725,-0.01659158,-0.00552411,0.01312641,-0.01290403,0.00226913,-0.00512796,-0.07437409,0.00387599,-0.01095711,0.04692162,0.0177956,0.0152691,-0.03206786,-0.00255734,0.0222334,-0.01323469,0.02343848,0.04654743,-0.02200069,0.06538053,0.04567805,0.03722672,0.06847561,0.07159495,0.05936934,-0.07952525,0.00946475,-0.04247935,0.00369655,0.06875399,0.00976531,-0.0240552,-0.0313462,0.03029393,-0.0258045,-0.00413549,-0.03558939,-0.03081363,0.08433414,-0.04375213,-0.06623513,-0.0523366,-0.03391096,-0.01558295,0.08832566,0.00523252,-0.06417896,0.00095631,0.05448998,0.05145149,-0.03918234,-0.1295595,-0.032952,-0.00672658,0.00328156,-0.07105223,0.06876891,0.01790138,-0.04263239,-0.05699196,0.02068214,0.01709668,-0.07556284,0.0319749,-0.03688223,-0.0463115,0.01202872,0.04993654,0.02262571,-0.02292398,0.00045999,-0.0365397,0.06254687,-0.01127847,-0.07202748,0.02207995,0.02266143,-0.00073259,-0.04857692,-0.00438762,-0.06536505,0.00239122,0.07004692,-0.01159281,0.05731421,-0.04815191,0.02623992,-0.06587864,-0.01274602,-0.05650322,-0.00058261,-0.0316132,-0.00900783,0.04836628,0.05102304,0.01998717,0.05889676,-0.01933491,0.00791066,0.00219031,-0.0057175,0.05749278,0.01888919,-0.05379655,0.02667693,0.07516993,-0.00919639,-0.01661063,0.00915839,0.01285286,0.05585995,-0.00949478,0.02123645,-0.03676927,-0.04938425,-0.04656788,-0.22688463,0.01579025,0.06069332,-0.01458397,0.05998823,-0.09083272,0.03924415,-0.04778542,0.06153089,0.09127784,0.08044051,-0.05592696,-0.03139133,0.05915624,-0.02618532,0.01154447,-0.031418,0.03518917,0.00779562,0.0300533,-0.0010075,0.00747282,0.07259437,-0.08081312,0.01116546,-0.01247875,0.15033168,0.00283015,0.09503285,0.06634196,0.02115255,0.02955418,-0.01195086,-0.03688933,0.05247709,-0.03946967,0.02346816,0.00718811,0.04924993,0.00329942,-0.03398125,-0.06519708,-0.02341204,-0.05528547,-0.01437516,-0.02284587,-0.06297932,-0.01325783,-0.01959674,0.09481309,0.0434145,0.01201605,0.08969261,0.02045007,0.00598515,-0.07218494,-0.06552821,-0.0363842,-0.02851279,-0.0032264,0.00370652,0.02036786,0.00341304,-0.07386475,-0.05253531,0.03295741,-0.01106009,0.02574085,-0.00874997,0.01076051,-0.02029395,0.12233496,0.04132849,0.03140526,0.0309431,0.10353249,-0.01127664,-0.04518049,-0.02883353,-0.00850636,0.03139925,-0.02336004,0.07906992,0.00983816,0.07312462,0.05519188,0.04355225,0.01647241,0.00229709,0.03440392,-0.08528947,-0.0106777,0.00135011,0.01769986,-0.01568221,-0.07870988,-0.2743744,-0.00949747,0.02167262,0.01303891,0.02346174,0.01492707,0.03058717,-0.06042397,-0.01186594,-0.02628995,-0.04050172,-0.00896117,0.03317536,-0.03734666,-0.02066903,0.0386128,0.06852847,-0.05690755,0.04503319,-0.05413143,0.01168238,-0.02945358,0.19733781,0.01292641,-0.00353101,-0.0236994,-0.05024641,-0.00077061,0.04673374,0.01625431,0.01291213,-0.00435503,0.09727032,-0.00814018,0.02642426,0.0416747,-0.00448268,0.02164922,0.0563067,0.01749067,0.04470001,0.05813288,-0.01365289,-0.03265153,0.11949315,0.01980363,-0.0268034,-0.04447708,-0.06140478,0.04231605,0.01072614,-0.00665786,-0.01871038,-0.00817721,0.05500405,0.0433731,-0.08524054,0.01089373,-0.02313098,0.01436617,-0.01964879,-0.02301869,0.00755063,0.00595985,0.00940805],"last_embed":{"hash":"6wcof4","tokens":446}}},"text":null,"length":0,"last_read":{"hash":"6wcof4","at":1768089443534},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#5.3 Transformers Decoder#{1}","lines":[86,127],"size":4267,"outlinks":[{"title":" 333","target":"Pasted image 20250315091058.png# left","line":2,"embedded":true},{"title":" 203","target":"Pasted image 20250316072836.png","line":2,"embedded":true},{"title":"Pasted image 20250316072647.png","target":"Pasted image 20250316072647.png","line":14,"embedded":true},{"title":"Pasted image 20250316073303.png","target":"Pasted image 20250316073303.png","line":19,"embedded":true},{"title":"Pasted image 20250315094502.png","target":"Pasted image 20250315094502.png","line":21,"embedded":true},{"title":"Pasted image 20250315095301.png","target":"Pasted image 20250315095301.png","line":24,"embedded":true},{"title":"Pasted image 20250315095948.png","target":"Pasted image 20250315095948.png","line":27,"embedded":true},{"title":"Pasted image 20250316063403.png","target":"Pasted image 20250316063403.png","line":30,"embedded":true},{"title":"Pasted image 20250315184141.png","target":"Pasted image 20250315184141.png","line":35,"embedded":true},{"title":"Pasted image 20250316062619.png","target":"Pasted image 20250316062619.png","line":42,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"6wcof4","at":1768089443534}},
"smart_blocks:Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#Side Note: LoRA#{1}": {"path":null,"embeddings":{"TaylorAI/bge-micro-v2":{"vec":[-0.06718817,-0.03808529,0.02416638,-0.01697952,-0.04939134,0.00910354,-0.04704038,0.0623057,0.03823954,0.01749593,0.06004111,-0.06898811,0.02996472,0.02889268,0.02627402,-0.0109907,-0.02327595,0.08730849,-0.01373683,-0.07085349,0.11657082,-0.04220973,-0.02848172,0.00547337,0.06701863,0.02311446,-0.02435054,0.04676437,-0.04514598,-0.26057529,0.00156196,0.01800642,0.06578531,0.03650749,-0.01496669,-0.0185734,-0.06952459,0.02463083,-0.06647281,-0.01154397,-0.00976153,-0.00537629,0.01288545,-0.0066795,0.01676654,-0.07028507,-0.03478784,0.01044269,-0.00270218,-0.00880274,0.03020978,-0.00041295,-0.02561616,0.03814777,0.02926552,0.04551891,0.13636729,0.02014662,0.05443252,0.05376797,0.00439459,0.06125942,-0.23235779,0.08784648,0.00160975,0.00467059,-0.06260066,-0.08494674,0.04635366,0.06522834,-0.06153632,-0.01961994,0.04100206,0.00394023,0.06861801,-0.0117664,-0.02557328,0.01139512,-0.02367205,-0.02434803,0.0353777,0.08851494,-0.01897009,0.00061088,-0.04064272,-0.02793273,-0.01212474,-0.05991881,-0.0157559,-0.00802151,-0.02008455,-0.00991984,-0.00005934,0.06902701,-0.08839748,-0.07716382,0.04552598,0.03752258,-0.07255293,0.11068245,-0.01316407,-0.02286125,0.00305359,0.00253497,0.03068777,-0.01387263,-0.01714823,-0.01370777,0.01550188,0.00804668,-0.01114723,0.02285399,-0.06161078,-0.09215242,-0.02202095,-0.08802145,0.04007893,0.03460763,-0.05062293,-0.03560825,-0.05184473,0.01238108,0.01963902,0.00646119,0.03791659,-0.00790155,0.04741459,0.07086828,0.03997692,0.10756791,0.0381152,0.04616085,-0.04323003,-0.01150667,-0.00222352,-0.01803109,0.05624003,-0.03259448,-0.00894288,0.01766257,-0.03646107,-0.06402337,-0.01054618,-0.01345435,-0.06300846,0.09773427,-0.07354105,-0.04907834,-0.02375332,-0.01889848,-0.02666619,0.04015228,-0.02177706,-0.00137744,0.01573947,-0.00058713,0.02086736,0.00100029,-0.11355145,0.03325028,-0.01033615,-0.07137144,-0.06644694,0.06087679,-0.00197784,-0.04269237,-0.02107507,0.04053292,0.05148307,-0.00025988,0.07375299,-0.00352882,-0.06645982,-0.00764986,0.07597931,0.05121581,-0.02709801,-0.01640213,0.0447832,0.06271769,0.0301514,-0.01482079,0.03178388,0.00949497,-0.02434468,-0.0481233,-0.02951873,-0.0266531,-0.02650247,-0.01052586,-0.09403025,0.03568122,-0.03608828,-0.0110718,-0.05499149,-0.00670457,-0.06988728,-0.04014545,-0.00108856,-0.03993998,0.00087613,0.00707315,0.00751489,0.00005349,0.00041115,0.01125079,0.06213701,-0.01527438,0.01940128,0.03748791,-0.03833931,0.00403611,0.09854735,-0.0153613,-0.0258651,0.04573857,0.05675466,0.0212473,-0.00126159,-0.01041411,0.03678364,-0.04028824,-0.05327015,-0.20276099,0.00202765,0.01903899,0.00756337,0.04372194,-0.08653476,0.02984909,-0.01679233,-0.00031363,0.09457526,0.05371304,-0.02105536,-0.05556902,0.07373817,-0.01763485,-0.00598007,-0.01739757,0.0511397,-0.01953417,-0.04818264,-0.00008025,0.01374026,0.01732032,-0.09391907,0.05047458,0.03108937,0.1680413,-0.00682289,0.03824602,0.07027058,0.01496174,-0.01396938,0.01105659,0.00110955,0.06339309,-0.02100305,-0.00726947,-0.06365178,-0.016086,-0.0307468,0.00694096,0.02530957,-0.04333506,-0.0717563,-0.00815802,0.01314865,-0.03530257,0.00279397,-0.04079566,0.0868533,0.04447287,-0.02960943,0.06234172,0.03355677,0.02150101,-0.01445509,-0.08513448,0.00301516,-0.01016868,0.01196713,-0.05922314,-0.03425883,0.00600319,-0.0865956,-0.0157839,-0.02216772,-0.01010313,0.01538916,-0.02302695,0.02099806,0.00256797,0.07771916,0.0512514,0.00815895,0.04240454,0.05980369,-0.00657556,-0.02036824,-0.02916682,0.02146848,0.07298556,-0.02072052,0.07258127,0.04792764,0.07687228,0.00760025,0.05276319,-0.03196162,0.04810728,-0.01370319,-0.05459134,-0.03095675,-0.02921908,0.04832159,0.05384108,-0.07763518,-0.26935312,0.03973583,0.03659907,0.06989716,0.00219941,-0.01368086,0.0177222,-0.03239429,-0.01697633,-0.00678334,-0.05007009,0.06690915,0.06617997,-0.0108978,0.02354419,-0.01556592,0.0609274,-0.08538326,0.03185125,0.0077019,-0.05837498,0.00265161,0.17767462,-0.01361693,0.01730164,-0.02883205,-0.01163058,-0.00513743,0.02803255,0.02238174,0.01619719,-0.01887795,0.06814019,-0.02695875,0.03703824,0.10633774,-0.02531437,0.03370682,0.04265336,-0.00212036,0.02070639,0.08646689,-0.03814661,-0.00177102,0.10216672,-0.03689923,-0.01979864,-0.03275065,-0.01510525,-0.02126388,-0.0099743,0.03759091,-0.00650856,-0.03262482,0.02459683,0.0130529,-0.02749451,0.02695504,0.02529958,0.00328843,0.00325395,-0.04293707,0.02097807,0.01353241,0.02759209],"last_embed":{"hash":"zzsxbx","tokens":198}}},"text":null,"length":0,"last_read":{"hash":"zzsxbx","at":1768089443554},"key":"Transformer Model Introduction (Attention Is All You Need).md##Chapter 5: Transformer Models#Side Note: LoRA#{1}","lines":[133,144],"size":514,"outlinks":[{"title":"Pasted image 20251030204002.png","target":"Pasted image 20251030204002.png","line":1,"embedded":true},{"title":"Pasted image 20251030204952.png","target":"Pasted image 20251030204952.png","line":5,"embedded":true},{"title":" 433","target":"Pasted image 20251030205157.png","line":10,"embedded":true},{"title":" 433","target":"Pasted image 20251030205148.png","line":12,"embedded":true}],"class_name":"SmartBlock","last_embed":{"hash":"zzsxbx","at":1768089443554}},
