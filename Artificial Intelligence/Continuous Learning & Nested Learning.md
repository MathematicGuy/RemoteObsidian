**Continous Learning -** Learn constantly without forgetting.
 
**Nested Learning -** new ML paradigm (eg. Google HOPE architecture) by structuring models as nested optimization problems with different update speeds, mimicking brain memory to prevent catestrophic forgetting and build **"living memory" for AI** (good at Needle in a Haystack problem). 
	Inspired from the brain's layered memory consolidation (*fast for short-term*, *slow for long-term*)

LLM remain static after training. 
**All NN are associative memory system** that compress their own context flow. 

Gradient Descent with momemtum is indeed a low-level optimization process, where the memory is optimized by simple gradient descent algorithm. 


 